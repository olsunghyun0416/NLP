{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Install"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "source": [
    "# Evn"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import json\n",
    "import zipfile\n",
    "import math\n",
    "import copy\n",
    "import collections\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed initialize\n",
    "random_seed = 1234\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google drive mount\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data dir\n",
    "data_dir = '/content/drive/MyDrive/Data/nlp' # change '/content/drive/MyDrive/Data/nlp' to your drive path \n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# korean wiki dir\n",
    "kowiki_dir = os.path.join(data_dir, 'kowiki')\n",
    "if not os.path.exists(kowiki_dir):\n",
    "    os.makedirs(kowiki_dir)\n",
    "os.listdir(kowiki_dir)"
   ]
  },
  {
   "source": [
    "# Vocabualry & Config"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab loading\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(os.path.join(data_dir, 'ko_32000.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(vocab)  # number of vocabulary\n",
    "n_seq = 256  # number of sequence\n",
    "d_model = 256  # dimension of model"
   ]
  },
  {
   "source": [
    "# 모델링"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 문장\n",
    "sentences = [\n",
    "    '나는 오늘 기분이 좋아 나는 오늘 우울해',\n",
    "    '나는 오늘 행복해 나는 오늘 즐거워'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 문장을 띄어쓰기 단위로 분할\n",
    "words = []\n",
    "for sentence in sentences:\n",
    "    words.extend(sentence.split())\n",
    "\n",
    "# 중복 단어 제거\n",
    "words = list(dict.fromkeys(words))\n",
    "\n",
    "# 각 단어별 고유한 번호 부여\n",
    "word_to_id = {'[PAD]': 0, '[UNK]': 1, '[BOS]': 2, '[EOS]': 3}\n",
    "for word in words:\n",
    "    word_to_id[word] = len(word_to_id)\n",
    "\n",
    "# 각 숫자별 단어 부여\n",
    "id_to_word = {_id:word for word, _id in word_to_id.items()}\n",
    "\n",
    "word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습용 입력 데이터 생성\n",
    "# [[4, 5, 6, 7, 4, 5, 8], [4, 5, 9, 4, 5, 10]]\n",
    "train_ids = []\n",
    "for sentence in sentences:\n",
    "    train_ids.append([word_to_id[word] for word in sentence.split()])\n",
    "\n",
    "# id를 입력과 정답으로 저장\n",
    "# ([[2, 4, 5, 6, 7, 4, 5, 8], [2, 4, 5, 9, 4, 5, 10]],\n",
    "#  [[4, 5, 6, 7, 4, 5, 8, 3], [4, 5, 9, 4, 5, 10, 3]])\n",
    "train_inputs, train_labels = [], []\n",
    "for train_id in train_ids:\n",
    "    train_inputs.append([word_to_id['[BOS]']] + train_id)\n",
    "    train_labels.append(train_id + [word_to_id['[EOS]']])\n",
    "\n",
    "# 문장의 길이를 모두 동일하게 변경 (최대길이 8)\n",
    "for row in train_inputs:\n",
    "    row += [0] * (8 - len(row))\n",
    "\n",
    "# 문장의 길이를 모두 동일하게 변경 (최대길이 8)\n",
    "for row in train_labels:\n",
    "    row += [0] * (8 - len(row))\n",
    "\n",
    "# train inputs을 numpy array로 변환\n",
    "train_inputs = np.array(train_inputs)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "train_inputs, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어를 벡터로 변환\n",
    "# AttributeError: 'list' object has no attribute 'dtype' -> 위에서 train_inputs을 numpy의 array 형식으로 만들어주지 않으면 생기는 에러\n",
    "embedding = tf.keras.layers.Embedding(len(word_to_id), 4)\n",
    "hidden = embedding(train_inputs)  # (bs, n_seq, 4)\n",
    "hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 실행\n",
    "lstm = tf.keras.layers.LSTM(units=5, return_sequences=True)\n",
    "hidden = lstm(hidden)\n",
    "hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다음단어 예측\n",
    "dense = tf.keras.layers.Dense(len(word_to_id), activation=tf.nn.softmax)\n",
    "outputs = dense(hidden)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_vocab, d_model, n_seq):\n",
    "    \"\"\"\n",
    "    문장 유사도 비교 모델\n",
    "    :param n_vocab: vocabulary 단어 수\n",
    "    :param d_model: 단어를 의미하는 벡터의 차원 수\n",
    "    :param n_seq: 문장 길이 (단어 수)\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.layers.Input((n_seq,))  # (bs, n_seq)\n",
    "    #######################################################\n",
    "    # 입력 단어를 vector로 변환\n",
    "    embedding = tf.keras.layers.Embedding(n_vocab, d_model)\n",
    "    hidden = embedding(inputs)  # (bs, n_seq, d_model)\n",
    "    # LSTM\n",
    "    lstm = tf.keras.layers.LSTM(units=d_model * 2, return_sequences=True)\n",
    "    hidden = lstm(hidden)  # (bs, n_seq, d_model * 2)\n",
    "    # 다음단어 확률 분포\n",
    "    dense = tf.keras.layers.Dense(n_vocab, activation=tf.nn.softmax)\n",
    "    outputs = dense(hidden)\n",
    "    #######################################################\n",
    "    # 학습할 모델 선언\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "model = build_model(len(word_to_id), 4, 8)\n",
    "# 모델 내용 그래프 출력\n",
    "tf.keras.utils.plot_model(model, 'model.png', show_shapes=True)"
   ]
  },
  {
   "source": [
    "# Processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 내용 확인\n",
    "with zipfile.ZipFile(os.path.join(kowiki_dir, 'kowiki.txt.zip')) as z:\n",
    "    with z.open('kowiki.txt') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            line = line.decode('utf-8').strip()\n",
    "            print(line)\n",
    "            if i >= 100:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 내용 확인 (주제단위)\n",
    "with zipfile.ZipFile(os.path.join(kowiki_dir, 'kowiki.txt.zip')) as z:\n",
    "    with z.open('kowiki.txt') as f:\n",
    "        doc = []\n",
    "        for i, line in enumerate(f):\n",
    "            line = line.decode('utf-8').strip()\n",
    "            if len(line) == 0:\n",
    "                if len(doc) > 0:\n",
    "                    break\n",
    "            else:\n",
    "                doc.append(line)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_instance(vocab, n_seq, doc):\n",
    "    \"\"\"\n",
    "    create train instance\n",
    "    :param vocab: vocabulary object\n",
    "    :param n_seq: sequece number\n",
    "    :param doc: wiki document\n",
    "    :return: train instance list\n",
    "    \"\"\"\n",
    "    n_max = n_seq - 1  # [BOS] or [EOS]\n",
    "    instance_list = []\n",
    "\n",
    "    chunk = []\n",
    "    chunk_len = 0\n",
    "    for i, line in enumerate(doc):\n",
    "        ##################################\n",
    "        # print(line)\n",
    "        tokens = vocab.encode_as_pieces(line)\n",
    "        chunk.append(tokens)\n",
    "        chunk_len += len(tokens)\n",
    "        if n_max <= chunk_len or i >= len(doc) -1:\n",
    "            # print()\n",
    "            # print(chunk_len, chunk)\n",
    "            # 문장의 배열을 하나의 배열로 변경\n",
    "            instance = []\n",
    "            for tokens in chunk:\n",
    "                instance.extend(tokens)\n",
    "            # 길이가 긴 경우 길이를 자름\n",
    "            instance = instance[:n_max]\n",
    "            # print(len(instance), instance)\n",
    "            # instance 저장\n",
    "            instance_list.append(instance)\n",
    "            # chunk 초기화\n",
    "            chunk = []\n",
    "            chunk_len = 0\n",
    "        ##################################\n",
    "    return instance_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instance 동작 확인\n",
    "instance_list = create_train_instance(vocab, n_seq, doc)\n",
    "for instance in instance_list:\n",
    "    print(len(instance), instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instance를 json 형태로 저장하는 함수\n",
    "def save_instance(vocab, n_seq, doc, o_f):\n",
    "    instance_list = create_train_instance(vocab, n_seq, doc)\n",
    "    for instance in instance_list:\n",
    "        o_f.write(json.dumps({'token': instance}, ensure_ascii=False))\n",
    "        o_f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 문서에 대한 instance 생성\n",
    "with open(os.path.join(kowiki_dir, 'kowiki_lm.json'), 'w') as o_f:\n",
    "    with zipfile.ZipFile(os.path.join(kowiki_dir, 'kowiki.txt.zip')) as z:\n",
    "        with z.open('kowiki.txt') as f:\n",
    "            doc = []\n",
    "            for i, line in enumerate(tqdm(f)):\n",
    "                line = line.decode('utf-8').strip()\n",
    "                if len(line) == 0:\n",
    "                    if len(doc) > 0:\n",
    "                        save_instance(vocab, n_seq, doc, o_f)\n",
    "                        doc = []\n",
    "                        # break\n",
    "                else:\n",
    "                    doc.append(line)\n",
    "            if len(doc) > 0:\n",
    "                save_instance(vocab, n_seq, doc, o_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 라인수 확인\n",
    "n_line = 0\n",
    "with open(os.path.join(kowiki_dir, 'kowiki_lm.json')) as f:\n",
    "    for line in f:\n",
    "        n_line += 1\n",
    "        if n_line <= 10:\n",
    "            print(line)\n",
    "n_line"
   ]
  },
  {
   "source": [
    "# Sample Data Project"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 1. Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(vocab, n_seq):\n",
    "    \"\"\"\n",
    "    Language Model 학습 데이터 생성\n",
    "    :param vocab: vocabulary object\n",
    "    :param n_seq: number of sequence\n",
    "    :return inputs_1: input data 1\n",
    "    :return inputs_2: input data 2\n",
    "    :return labels: label data\n",
    "    \"\"\"\n",
    "    # line 수 조회\n",
    "    # n_line = 0\n",
    "    # with open(os.path.join(kowiki_dir, 'kowiki_lm.json')) as f:\n",
    "    #     for line in f:\n",
    "    #         n_line += 1\n",
    "    # 10개로 제한\n",
    "    n_data = 10\n",
    "    # 빈 데이터 생성\n",
    "    inputs = np.zeros((n_data, n_seq)).astype(np.int32)\n",
    "    labels = np.zeros((n_data, n_seq)).astype(np.int32)\n",
    "\n",
    "    with open(os.path.join(kowiki_dir, 'kowiki_lm.json')) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= n_data:\n",
    "                break\n",
    "            ##################################\n",
    "            data = json.loads(line)\n",
    "            print()\n",
    "            print(data)\n",
    "            token_id = [vocab.piece_to_id(p) for p in data['token']]\n",
    "            print(len(token_id), token_id)\n",
    "            # input id\n",
    "            input_id = [vocab.bos_id()] + token_id\n",
    "            input_id += [0] * (n_seq - len(input_id))\n",
    "            print(len(input_id), input_id)\n",
    "            # label id\n",
    "            label_id = token_id + [vocab.eos_id()]\n",
    "            label_id += [0] * (n_seq - len(label_id))\n",
    "            print(len(label_id), label_id)\n",
    "            # 값 저장\n",
    "            inputs[i] = input_id\n",
    "            labels[i] = label_id\n",
    "            ##################################\n",
    "\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data 생성\n",
    "train_inputs, train_labels = load_data(vocab, n_seq)\n",
    "train_inputs, train_labels"
   ]
  },
  {
   "source": [
    "# 2. Loss & Acc"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4개 의 단어를 예측하기 위한 정답과 예측값 가정\n",
    "# 정답\n",
    "y_true = np.random.randint(1, 3, (1, 20)).astype(np.float32)\n",
    "y_true[:, 8:] = 0\n",
    "# 예측 값\n",
    "y_pred = np.random.random((1, 20, 4)).astype(np.float32)\n",
    "y_pred = tf.nn.softmax(y_pred, axis=-1).numpy()  # 확률 값으로 변경\n",
    "\n",
    "y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    pad 부분을 제외하고 loss를 계산하는 함수\n",
    "    :param y_true: 정답\n",
    "    :param y_pred: 예측 값\n",
    "    :retrun loss: pad 부분이 제외된 loss 값\n",
    "    \"\"\"\n",
    "    # sparse_entropy = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    sparse_entropy = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    loss = sparse_entropy(y_true, y_pred)\n",
    "    # mask 계산\n",
    "    # print(y_true)\n",
    "    mask = tf.not_equal(y_true, 0)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    # print(mask)\n",
    "    loss *= mask\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm_loss 함수 확인\n",
    "loss = lm_loss(y_true, y_pred)\n",
    "print(loss, np.mean(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    pad 부분을 제외하고 accuracy를 계산하는 함수\n",
    "    :param y_true: 정답\n",
    "    :param y_pred: 예측 값\n",
    "    :retrun loss: pad 부분이 제외된 accuracy 값\n",
    "    \"\"\"\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    # print(y_true)\n",
    "    y_pred_class = tf.cast(tf.argmax(y_pred, axis=-1), tf.float32)\n",
    "    # print(y_pred_class)\n",
    "    matches = tf.cast(tf.equal(y_true, y_pred_class), tf.float32)\n",
    "    # print(matches)\n",
    "    # mask 계산\n",
    "    mask = tf.not_equal(y_true, 0)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    # print(mask)\n",
    "    # match에 mask 적용\n",
    "    matches *= mask\n",
    "    # print(matches)\n",
    "    # accuracy = tf.reduce_sum(matches) / tf.maximum(tf.reduce_sum(tf.ones_like(matches)), 1)\n",
    "    # 분모는 mask의 1인 개수\n",
    "    accuracy = tf.reduce_sum(matches) / tf.maximum(tf.reduce_sum(mask), 1)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm_acc 함수 확인\n",
    "accuracy = lm_acc(y_true, y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "source": [
    "# 학습"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "model = build_model(len(vocab), d_model, n_seq)\n",
    "# 모델 내용 그래프 출력\n",
    "tf.keras.utils.plot_model(model, 'model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 loss, optimizer, metric 정의\n",
    "model.compile(loss=lm_loss, optimizer='adam', metrics=[lm_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='lm_acc', patience=50)\n",
    "# save weights callback\n",
    "save_weights = tf.keras.callbacks.ModelCheckpoint(os.path.join(kowiki_dir, 'lm.hdf5'),\n",
    "                                                  monitor='lm_acc',\n",
    "                                                  verbose=1,\n",
    "                                                  save_best_only=True,\n",
    "                                                  mode=\"max\",\n",
    "                                                  save_freq=\"epoch\",\n",
    "                                                  save_weights_only=True)\n",
    "# csv logger\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(os.path.join(kowiki_dir, 'lm.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "history = model.fit(train_inputs,\n",
    "                    train_labels,\n",
    "                    epochs=500,\n",
    "                    batch_size=128,\n",
    "                    callbacks=[early_stopping, save_weights, csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], 'b-', label='loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['lm_acc'], 'g-', label='acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "# Inference"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "model = build_model(len(vocab), d_model, n_seq)\n",
    "# train weight로 초기화\n",
    "model.load_weights(os.path.join(kowiki_dir, 'lm.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_next(vocab, model, n_seq, string):\n",
    "    \"\"\"\n",
    "    다음단어 예측\n",
    "    :param vocab: vocab\n",
    "    :param model: model\n",
    "    :param n_seq: number of seqence\n",
    "    :param string: inpust string\n",
    "    \"\"\"\n",
    "    n_max = n_seq - 1\n",
    "    \n",
    "    tokens = vocab.encode_as_pieces(string)\n",
    "    start_idx = len(tokens)\n",
    "    token_id = [vocab.piece_to_id(p) for p in tokens][:n_max]\n",
    "    token_id = [vocab.bos_id()] + token_id\n",
    "    token_id += [0] * (n_seq - len(token_id))\n",
    "    assert len(token_id) == n_seq\n",
    "    print(tokens)\n",
    "    print(start_idx, ':', token_id)\n",
    "\n",
    "    ###################################\n",
    "    result = model.predict(np.array([token_id]))\n",
    "    print(result.shape)\n",
    "    prob = result[0][start_idx]\n",
    "    print(prob)\n",
    "\n",
    "    # 정렬 (작 --> 큰)\n",
    "    max_args = np.argsort(prob)[-10:]\n",
    "    print(max_args)\n",
    "    max_args = list(max_args)\n",
    "    # 정렬 (큰 --> 작)\n",
    "    max_args.reverse()\n",
    "    print(max_args)\n",
    "\n",
    "    next_prob = []\n",
    "    for i in max_args:\n",
    "        w = vocab.id_to_piece(int(i))\n",
    "        p = prob[i]\n",
    "        next_prob.append((w, p))\n",
    "    ###################################\n",
    "    return next_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = '대한민국'\n",
    "do_next(vocab, model, n_seq, string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    string = input('시작 문장 > ')\n",
    "    string = string.strip()\n",
    "    if len(string) == 0:\n",
    "        break\n",
    "    next_prob = do_next(vocab, model, n_seq, string)\n",
    "    for w, p in next_prob:\n",
    "        print(f'{w}: {p}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_generate(vocab, model, n_seq, string):\n",
    "    \"\"\"\n",
    "    문장생성\n",
    "    :param vocab: vocab\n",
    "    :param model: model\n",
    "    :param n_seq: number of seqence\n",
    "    :param string: inpust string\n",
    "    \"\"\"\n",
    "    n_max = n_seq - 1\n",
    "    tokens = vocab.encode_as_pieces(string)\n",
    "    start_idx = len(tokens)\n",
    "    token_id = [vocab.piece_to_id(p) for p in tokens][:n_max]\n",
    "    token_id = [vocab.bos_id()] + token_id\n",
    "    token_id += [0] * (n_seq - len(token_id))\n",
    "    assert len(token_id) == n_seq\n",
    "\n",
    "    for _ in range(start_idx, n_seq - 1):\n",
    "        print(token_id)\n",
    "        #########################################\n",
    "        outputs = model.predict(np.array([token_id]))\n",
    "        prob = outputs[0][start_idx]\n",
    "        # word_id = int(np.argmax(prob))\n",
    "        word_id = int(np.random.choice(len(vocab), 1, p=prob)[0])\n",
    "        if word_id == vocab.eos_id():\n",
    "            break\n",
    "        token_id[start_idx + 1] = word_id\n",
    "        start_idx += 1\n",
    "        #########################################\n",
    "    predict_id = token_id[1:start_idx + 1]\n",
    "    predict_str = vocab.decode_ids(predict_id)\n",
    "    print(predict_id, predict_str)\n",
    "    return predict_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = '대한민국'\n",
    "do_generate(vocab, model, n_seq, string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    string = input('시작 문장 > ')\n",
    "    string = string.strip()\n",
    "    if len(string) == 0:\n",
    "        break\n",
    "    predict_str = do_generate(vocab, model, n_seq, string)\n",
    "    print(predict_str)"
   ]
  },
  {
   "source": [
    "# All Data Project"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 1. Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(vocab, n_seq):\n",
    "    \"\"\"\n",
    "    Language Model 학습 데이터 생성\n",
    "    :param vocab: vocabulary object\n",
    "    :param n_seq: number of sequence\n",
    "    :return inputs_1: input data 1\n",
    "    :return inputs_2: input data 2\n",
    "    :return labels: label data\n",
    "    \"\"\"\n",
    "    # line 수 조회\n",
    "    n_line = 0\n",
    "    with open(os.path.join(kowiki_dir, 'kowiki_lm.json')) as f:\n",
    "        for line in f:\n",
    "            n_line += 1\n",
    "    # 최대 100,000개 데이터\n",
    "    n_data = min(n_line, 100000)\n",
    "    # 빈 데이터 생성\n",
    "    inputs = np.zeros((n_data, n_seq)).astype(np.int32)\n",
    "    labels = np.zeros((n_data, n_seq)).astype(np.int32)\n",
    "\n",
    "    with open(os.path.join(kowiki_dir, 'kowiki_lm.json')) as f:\n",
    "        for i, line in enumerate(tqdm(f, total=n_data)):\n",
    "            if i >= n_data:\n",
    "                break\n",
    "            data = json.loads(line)\n",
    "            token_id = [vocab.piece_to_id(p) for p in data['token']]\n",
    "            # input id\n",
    "            input_id = [vocab.bos_id()] + token_id\n",
    "            input_id += [0] * (n_seq - len(input_id))\n",
    "            # label id\n",
    "            label_id = token_id + [vocab.eos_id()]\n",
    "            label_id += [0] * (n_seq - len(label_id))\n",
    "            # 값 저장\n",
    "            inputs[i] = input_id\n",
    "            labels[i] = label_id\n",
    "\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data 생성\n",
    "train_inputs, train_labels = load_data(vocab, n_seq)\n",
    "train_inputs, train_labels"
   ]
  },
  {
   "source": [
    "# 2. Loss & Acc"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    pad 부분을 제외하고 loss를 계산하는 함수\n",
    "    :param y_true: 정답\n",
    "    :param y_pred: 예측 값\n",
    "    :retrun loss: pad 부분이 제외된 loss 값\n",
    "    \"\"\"\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)(y_true, y_pred)\n",
    "    mask = tf.not_equal(y_true, 0)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    loss *= mask\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    pad 부분을 제외하고 accuracy를 계산하는 함수\n",
    "    :param y_true: 정답\n",
    "    :param y_pred: 예측 값\n",
    "    :retrun loss: pad 부분이 제외된 accuracy 값\n",
    "    \"\"\"\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred_class = tf.cast(tf.argmax(y_pred, axis=-1), tf.float32)\n",
    "    matches = tf.cast(tf.equal(y_true, y_pred_class), tf.float32)\n",
    "    mask = tf.not_equal(y_true, 0)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    matches *= mask\n",
    "    accuracy = tf.reduce_sum(matches) / tf.maximum(tf.reduce_sum(mask), 1)\n",
    "    return accuracy"
   ]
  },
  {
   "source": [
    "# 3. 학습"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 loss, optimizer, metric 정의\n",
    "model.compile(loss=lm_loss, optimizer='adam', metrics=[lm_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='lm_acc', patience=50)\n",
    "# save weights callback\n",
    "save_weights = tf.keras.callbacks.ModelCheckpoint(os.path.join(kowiki_dir, 'lm.hdf5'),\n",
    "                                                  monitor='lm_acc',\n",
    "                                                  verbose=1,\n",
    "                                                  save_best_only=True,\n",
    "                                                  mode=\"max\",\n",
    "                                                  save_freq=\"epoch\",\n",
    "                                                  save_weights_only=True)\n",
    "# csv logger\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(os.path.join(kowiki_dir, 'lm.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "history = model.fit(train_inputs,\n",
    "                    train_labels,\n",
    "                    epochs=2,\n",
    "                    batch_size=64,\n",
    "                    callbacks=[early_stopping, save_weights, csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], 'b-', label='loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['lm_acc'], 'g-', label='acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "# Inference"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "model = build_model(len(vocab), d_model, n_seq)\n",
    "# train weight로 초기화\n",
    "model.load_weights(os.path.join(kowiki_dir, 'lm.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_next(vocab, model, n_seq, string):\n",
    "    \"\"\"\n",
    "    다음단어 예측\n",
    "    :param vocab: vocab\n",
    "    :param model: model\n",
    "    :param n_seq: number of seqence\n",
    "    :param string: inpust string\n",
    "    \"\"\"\n",
    "    n_max = n_seq - 1\n",
    "    \n",
    "    tokens = vocab.encode_as_pieces(string)\n",
    "    start_idx = len(tokens)\n",
    "    token_id = [vocab.piece_to_id(p) for p in tokens][:n_max]\n",
    "    token_id = [vocab.bos_id()] + token_id\n",
    "    token_id += [0] * (n_seq - len(token_id))\n",
    "    assert len(token_id) == n_seq\n",
    "\n",
    "    result = model.predict(np.array([token_id]))\n",
    "    prob = result[0][start_idx]\n",
    "    max_args = np.argsort(prob)[-10:]\n",
    "    max_args = list(max_args)\n",
    "    max_args.reverse()\n",
    "\n",
    "    next_prob = []\n",
    "    for i in max_args:\n",
    "        w = vocab.id_to_piece(int(i))\n",
    "        p = prob[i]\n",
    "        next_prob.append((w, p))\n",
    "    return next_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    string = input('시작 문장 > ')\n",
    "    string = string.strip()\n",
    "    if len(string) == 0:\n",
    "        break\n",
    "    next_prob = do_next(vocab, model, n_seq, string)\n",
    "    for w, p in next_prob:\n",
    "        print(f'{w}: {p}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_generate(vocab, model, n_seq, string):\n",
    "    \"\"\"\n",
    "    문장생성\n",
    "    :param vocab: vocab\n",
    "    :param model: model\n",
    "    :param n_seq: number of seqence\n",
    "    :param string: inpust string\n",
    "    \"\"\"\n",
    "    n_max = n_seq - 1\n",
    "    tokens = vocab.encode_as_pieces(string)\n",
    "    start_idx = len(tokens)\n",
    "    token_id = [vocab.piece_to_id(p) for p in tokens][:n_max]\n",
    "    token_id = [vocab.bos_id()] + token_id\n",
    "    token_id += [0] * (n_seq - len(token_id))\n",
    "    assert len(token_id) == n_seq\n",
    "\n",
    "    for _ in range(start_idx, n_seq - 1):\n",
    "        outputs = model.predict(np.array([token_id]))\n",
    "        prob = outputs[0][start_idx]\n",
    "        word_id = int(np.random.choice(len(vocab), 1, p=prob)[0])\n",
    "        # word_id = np.argmax(prob)\n",
    "        if word_id == vocab.eos_id():\n",
    "            break\n",
    "        token_id[start_idx + 1] = word_id\n",
    "        start_idx += 1\n",
    "    predict_id = token_id[1:start_idx + 1]\n",
    "    predict_str = vocab.decode_ids(predict_id)\n",
    "    return predict_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    string = input('시작 문장 > ')\n",
    "    string = string.strip()\n",
    "    if len(string) == 0:\n",
    "        break\n",
    "    predict_str = do_generate(vocab, model, n_seq, string)\n",
    "    print(predict_str)"
   ]
  }
 ]
}