{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Evn"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import json\n",
    "import zipfile\n",
    "import math\n",
    "import copy\n",
    "import collections\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as KK\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed initialize\n",
    "random_seed = 1234\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "source": [
    "# Config"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    \"\"\"\n",
    "    json을 config 형태로 사용하기 위한 Class\n",
    "    :param dict: config dictionary\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "    print(__getattr__)\n",
    "    print(__setattr__)\n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        \"\"\"\n",
    "        file에서 Config를 생성 함\n",
    "        :param file: filename\n",
    "        \"\"\"\n",
    "        with open(file, 'r') as f:\n",
    "            config = json.loads(f.read())\n",
    "            return Config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config 생성\n",
    "# d_model: model hidden dim\n",
    "# n_head: multi head attention head number\n",
    "# d_head: multi head attention head dim\n",
    "# dropout: dropout rate\n",
    "# d_ff: feed forward dim\n",
    "# norm_eps: layernormal epsilon\n",
    "# n_layer: layer number\n",
    "# n_seq: sequence max number\n",
    "# n_vocab: vocab count\n",
    "# i_pad: vocab pad id\n",
    "config = Config({\"d_model\": 8,\n",
    "                 \"n_head\": 2,\n",
    "                 \"d_head\": 4,\n",
    "                 \"dropout\": 0.1,\n",
    "                 \"d_ff\": 32,\n",
    "                 \"norm_eps\": 0.001,\n",
    "                 \"n_layer\": 6,\n",
    "                 \"n_seq\": 16,\n",
    "                 \"n_vocab\": 16,\n",
    "                 \"i_pad\": 0})\n",
    "config"
   ]
  },
  {
   "source": [
    "# Input"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 문장\n",
    "sentences = [\n",
    "    ['나는 오늘 행복해', '나도 기분이 좋아'],\n",
    "    # ['나는 오늘 기분이 좋아', '나도 매우 행복하다'],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 문장을 띄어쓰기 단위로 분할\n",
    "words = []\n",
    "for pair in sentences:\n",
    "    for sentence in pair:\n",
    "        words.extend(sentence.split())\n",
    "\n",
    "# 중복 단어 제거\n",
    "words = list(dict.fromkeys(words))\n",
    "\n",
    "# 각 단어별 고유한 번호 부여\n",
    "word_to_id = {'[PAD]': 0, '[UNK]': 1, '[BOS]': 2, '[EOS]': 3}\n",
    "for word in words:\n",
    "    word_to_id[word] = len(word_to_id)\n",
    "\n",
    "# 각 숫자별 단어 부여\n",
    "id_to_word = {_id:word for word, _id in word_to_id.items()}\n",
    "\n",
    "word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question과 Answer를 숫자료\n",
    "question_list, answer_list = [], []\n",
    "\n",
    "for pair in sentences:\n",
    "    question_list.append([word_to_id[word] for word in pair[0].split()])\n",
    "    answer_list.append([word_to_id[word] for word in pair[1].split()])\n",
    "\n",
    "# 학습용 입력 데이터 생성\n",
    "train_enc_inputs, train_dec_inputs, train_labels = [], [], []\n",
    "for question, answer in zip(question_list, answer_list):\n",
    "    train_enc_inputs.append(question)\n",
    "    train_dec_inputs.append([word_to_id['[BOS]']] + answer)\n",
    "    train_labels.append(answer + [word_to_id['[EOS]']])\n",
    "\n",
    "# Encoder 입력의 길이를 모두 동일하게 변경 (최대길이 4)\n",
    "for row in train_enc_inputs:\n",
    "    row += [0] * (4 - len(row))\n",
    "\n",
    "# Decoder 입력의 길이를 모두 동일하게 변경 (최대길이 6)\n",
    "for row in train_dec_inputs:\n",
    "    row += [0] * (6 - len(row))\n",
    "\n",
    "# 정답의 길이를 모두 동일하게 변경 (최대길이 6)\n",
    "for row in train_labels:\n",
    "    row += [0] * (6 - len(row))\n",
    "\n",
    "# numpy array로 변환/\n",
    "train_enc_inputs = np.array(train_enc_inputs)\n",
    "train_dec_inputs = np.array(train_dec_inputs)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "train_enc_inputs, train_dec_inputs, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding with random weight\n",
    "embed_weight = np.random.randint(-9, 10, (config.n_vocab, config.d_model)) / 10\n",
    "\n",
    "embed = tf.keras.layers.Embedding(config.n_vocab, config.d_model, weights=[embed_weight])\n",
    "embed_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder hidden\n",
    "hidden_enc = embed(train_enc_inputs)\n",
    "hidden_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder hidden\n",
    "hidden_dec = embed(train_dec_inputs)\n",
    "hidden_dec"
   ]
  },
  {
   "source": [
    "# Mask"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 1. Pad Mask"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_enc_inputs, train_dec_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    pad mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: pad mask (pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    #########################################\n",
    "    # 0인 부분 확인\n",
    "    mask = tf.math.equal(tokens, i_pad)\n",
    "    # boolean -> float 32\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    # expand dimension for n_seq\n",
    "    mask = tf.expand_dims(mask, axis=1)\n",
    "    #########################################\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_pad_mask = get_pad_mask(train_enc_inputs)\n",
    "enc_pad_mask"
   ]
  },
  {
   "source": [
    "# 2. Causal Mask"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_causal_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    causal mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: causal and pad mask (causal or pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    #########################################\n",
    "    # 개수 조회\n",
    "    n_seq = tf.shape(tokens)[1]\n",
    "    # print(n_seq)\n",
    "    # make ahead mask\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)\n",
    "    # expand dim for bs\n",
    "    mask = tf.expand_dims(mask, axis=0)\n",
    "    # print(mask)\n",
    "    # get pad_mask\n",
    "    pad_mask = get_pad_mask(tokens, i_pad)\n",
    "    # print(pad_mask)\n",
    "    # mask all ahead_mask or pad_mask\n",
    "    mask = tf.maximum(mask, pad_mask)\n",
    "    #########################################\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_causal_mask = get_causal_mask(train_dec_inputs)\n",
    "dec_causal_mask"
   ]
  },
  {
   "source": [
    "# Mask 생성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Self Attetnion mask\n",
    "enc_self_mask = get_pad_mask(train_enc_inputs)\n",
    "enc_self_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder Self Attetnion mask\n",
    "dec_self_mask = get_causal_mask(train_dec_inputs)\n",
    "dec_self_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder-Decoder Attetnion mask\n",
    "enc_dec_mask = get_pad_mask(train_enc_inputs)\n",
    "enc_dec_mask"
   ]
  },
  {
   "source": [
    "# Scaled dot product attention"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleDotProductAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Scale Dot Product Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"scale_dot_product_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: Q, K, V, attn_mask tuple\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        #########################################\n",
    "        Q, K, V, attn_mask = inputs\n",
    "        # matmul Q, K (transpose_b=True)\n",
    "        attn_score = tf.matmul(Q, K, transpose_b=True)\n",
    "        # get scale = d_model ** 0.5\n",
    "        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n",
    "        # print(attn_score)\n",
    "        # divide by scale\n",
    "        attn_scale = tf.math.divide(attn_score, scale)\n",
    "        # print(attn_scale)\n",
    "        # do mask (subtract 1e-9 for masked value)\n",
    "        attn_scale -= 1.e9 * attn_mask\n",
    "        # print(attn_scale)\n",
    "        # calculate attention prob\n",
    "        attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n",
    "        # print(attn_prob)\n",
    "        # weighted sum of V\n",
    "        attn_out = tf.matmul(attn_prob, V)\n",
    "        return attn_out\n",
    "        #########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Self Attetnion\n",
    "Q = hidden_enc\n",
    "K = hidden_enc\n",
    "V = hidden_enc\n",
    "\n",
    "attention = ScaleDotProductAttention()\n",
    "attn_out = attention((Q, K, V, enc_self_mask))\n",
    "attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder Self Attetnion\n",
    "Q = hidden_dec\n",
    "K = hidden_dec\n",
    "V = hidden_dec\n",
    "\n",
    "attn_out = attention((Q, K, V, dec_self_mask))\n",
    "attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder-Decoder Attetnion\n",
    "Q = hidden_dec\n",
    "K = hidden_enc\n",
    "V = hidden_enc\n",
    "\n",
    "attn_out = attention((Q, K, V, enc_dec_mask))\n",
    "attn_out"
   ]
  },
  {
   "source": [
    "# Multi Head Attention"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi Head Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"multi_head_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.d_model = config.d_model\n",
    "        self.n_head = config.n_head\n",
    "        self.d_head = config.d_head\n",
    "\n",
    "        # Q, K, V input dense layer\n",
    "        self.W_Q = tf.keras.layers.Dense(config.n_head * config.d_head)\n",
    "        self.W_K = tf.keras.layers.Dense(config.n_head * config.d_head)\n",
    "        self.W_V = tf.keras.layers.Dense(config.n_head * config.d_head)\n",
    "        # Scale Dot Product Attention class\n",
    "        self.attention = ScaleDotProductAttention(name=\"self_attention\")\n",
    "        # output dense layer\n",
    "        self.W_O = tf.keras.layers.Dense(config.d_model)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: Q, K, V, attn_mask tuple\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        #########################################\n",
    "        Q, K, V, attn_mask = inputs\n",
    "        # Q_m = self.W_Q(Q)\n",
    "        # print(Q_m.shape)\n",
    "        # Q_m = tf.reshape(Q_m, [-1, tf.shape(Q)[1], self.n_head, self.d_head])\n",
    "        # print(Q_m.shape)\n",
    "        # Q_m = tf.transpose(Q_m, [0, 2, 1, 3])\n",
    "        # print(Q_m.shape)\n",
    "        # build multihead Q, K, V\n",
    "        Q_m = tf.transpose(tf.reshape(self.W_Q(Q), [-1, tf.shape(Q)[1], self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, Q_len, d_head)\n",
    "        K_m = tf.transpose(tf.reshape(self.W_K(K), [-1, tf.shape(K)[1], self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        V_m = tf.transpose(tf.reshape(self.W_V(V), [-1, tf.shape(V)[1], self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        # print(Q_m.shape, K_m.shape, V_m.shape)÷\n",
    "        # build multihead mask\n",
    "        attn_mask_m = tf.expand_dims(attn_mask, axis=1)\n",
    "        # print(attn_mask.shape, attn_mask_m.shape)\n",
    "        # Scale Dot Product Attention with multi head Q, K, V, attn_mask\n",
    "        attn_out_m = self.attention((Q_m, K_m, V_m, attn_mask_m))  # (bs, n_head, Q_len, d_head)\n",
    "        # print(attn_out_m.shape)\n",
    "        # transpose and reshape\n",
    "        attn_out_t = tf.transpose(attn_out_m, perm=[0, 2, 1, 3])  # (bs, Q_len, n_head, d_head)\n",
    "        # print(attn_out_t.shape)\n",
    "        attn_out_c = tf.reshape(attn_out_t, [-1, tf.shape(Q)[1], config.n_head * config.d_head])  # (bs, Q_len, n_head * d_head)\n",
    "        # print(attn_out_c.shape)\n",
    "        # linear for output\n",
    "        attn_out = self.W_O(attn_out_c) # (bs, Q_len, d_model)\n",
    "        return attn_out\n",
    "        #########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Self Attetnion\n",
    "Q = hidden_enc\n",
    "K = hidden_enc\n",
    "V = hidden_enc\n",
    "\n",
    "attention = MultiHeadAttention(config)\n",
    "attn_out = attention((Q, K, V, enc_self_mask))\n",
    "attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder Self Attetnion\n",
    "Q = hidden_dec\n",
    "K = hidden_dec\n",
    "V = hidden_dec\n",
    "\n",
    "attn_out = attention((Q, K, V, dec_self_mask))\n",
    "attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder-Decoder Attetnion\n",
    "Q = hidden_dec\n",
    "K = hidden_enc\n",
    "V = hidden_enc\n",
    "\n",
    "attn_out = attention((Q, K, V, enc_dec_mask))\n",
    "attn_out"
   ]
  }
 ]
}