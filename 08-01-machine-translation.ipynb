{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 한글 폰트 설치"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 폰트 설치\n",
    "!apt-get update -qq # 나눔고딕 설치\n",
    "!apt-get install fonts-nanum* -qq\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# 폰트 로딩\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager._rebuild()"
   ]
  },
  {
   "source": [
    "# Install"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "source": [
    "# Evn"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import json\n",
    "import zipfile\n",
    "import math\n",
    "import copy\n",
    "import collections\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed initialize\n",
    "random_seed = 1234\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google drive mount\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data dir\n",
    "data_dir = '/content/drive/MyDrive/Data/nlp'\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# songys chatbot dir\n",
    "songys_dir = os.path.join(data_dir, 'songys')\n",
    "if not os.path.exists(songys_dir):\n",
    "    os.makedirs(songys_dir)\n",
    "os.listdir(songys_dir)"
   ]
  },
  {
   "source": [
    "# 모델링"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 문장\n",
    "sentences = [\n",
    "    ['나는 오늘 기분이 좋아', '네가 기분이 좋으니 나도 좋아'],\n",
    "    ['나는 오늘 행복해', '나도 행복하다'],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 문장을 띄어쓰기 단위로 분할\n",
    "words = []\n",
    "for pair in sentences:\n",
    "    for sentence in pair:\n",
    "        words.extend(sentence.split())\n",
    "\n",
    "# 중복 단어 제거\n",
    "words = list(dict.fromkeys(words))\n",
    "\n",
    "# 각 단어별 고유한 번호 부여\n",
    "word_to_id = {'[PAD]': 0, '[UNK]': 1, '[BOS]': 2, '[EOS]': 3}\n",
    "for word in words:\n",
    "    word_to_id[word] = len(word_to_id)\n",
    "\n",
    "# 각 숫자별 단어 부여\n",
    "id_to_word = {_id:word for word, _id in word_to_id.items()}\n",
    "\n",
    "word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question과 Answer를 숫자료\n",
    "question_list, answer_list = [], []\n",
    "\n",
    "for pair in sentences:\n",
    "    question_list.append([word_to_id[word] for word in pair[0].split()])\n",
    "    answer_list.append([word_to_id[word] for word in pair[1].split()])\n",
    "\n",
    "question_list, answer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습용 입력 데이터 생성\n",
    "train_enc_inputs, train_dec_inputs, train_labels = [], [], []\n",
    "\n",
    "for question, answer in zip(question_list, answer_list):\n",
    "    train_enc_inputs.append(question)\n",
    "    train_dec_inputs.append([word_to_id['[BOS]']] + answer)\n",
    "    train_labels.append(answer + [word_to_id['[EOS]']])\n",
    "\n",
    "train_enc_inputs, train_dec_inputs, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder 입력의 길이를 모두 동일하게 변경 (최대길이 4)\n",
    "for row in train_enc_inputs:\n",
    "    row += [0] * (4 - len(row))\n",
    "\n",
    "# Decoder 입력의 길이를 모두 동일하게 변경 (최대길이 6)\n",
    "for row in train_dec_inputs:\n",
    "    row += [0] * (6 - len(row))\n",
    "\n",
    "# 정답의 길이를 모두 동일하게 변경 (최대길이 6)\n",
    "for row in train_labels:\n",
    "    row += [0] * (6 - len(row))\n",
    "\n",
    "train_enc_inputs, train_dec_inputs, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy array로 변환\n",
    "train_enc_inputs = np.array(train_enc_inputs)\n",
    "train_dec_inputs = np.array(train_dec_inputs)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "train_enc_inputs, train_dec_inputs, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding 생성\n",
    "embedding = tf.keras.layers.Embedding(len(id_to_word), 5)\n",
    "\n",
    "# word embedding\n",
    "hidden_enc = embedding(train_enc_inputs)  # (bs, n_enc_seq, 5)\n",
    "hidden_dec = embedding(train_dec_inputs)  # (bs, n_dec_seq, 5)\n",
    "hidden_enc, hidden_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder LSTM (uni-direction, bi-direction 가능)\n",
    "fw_cell = tf.keras.layers.LSTM(units=4, return_state=True)\n",
    "bw_cell = tf.keras.layers.LSTM(units=4, go_backwards=True, return_state=True)\n",
    "\n",
    "lstm_enc = tf.keras.layers.Bidirectional(fw_cell, backward_layer=bw_cell)\n",
    "hidden_enc, fw_h, fw_c, bw_h, bw_c = lstm_enc(hidden_enc)\n",
    "\n",
    "hidden_enc.shape, fw_h.shape, fw_c.shape, bw_h.shape, bw_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat hidden and cell state\n",
    "s_h = K.concatenate([fw_h, bw_h], axis=-1)\n",
    "s_c = K.concatenate([fw_c, bw_c], axis=-1)\n",
    "s_h.shape, s_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder LSTM (uni-direction만 가능)\n",
    "lstm_dec = tf.keras.layers.LSTM(units=8, return_sequences=True)\n",
    "hidden_dec = lstm_dec(hidden_dec, initial_state=[s_h, s_c])\n",
    "hidden_dec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder 입력의 다음단어 예측\n",
    "dense_out = tf.keras.layers.Dense(units=len(id_to_word), activation=tf.nn.softmax)\n",
    "outputs = dense_out(hidden_dec)\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_vocab, d_model, n_enc_seq, n_dec_seq):\n",
    "    \"\"\"\n",
    "    seq2seq 모델\n",
    "    :param n_vocab: vocabulary 단어 수\n",
    "    :param d_model: 단어를 의미하는 벡터의 차원 수\n",
    "    :param n_enc_seq: encoder 문장 길이 (단어 수)\n",
    "    :param n_dec_seq: decoder 문장 길이 (단어 수)\n",
    "    \"\"\"\n",
    "    inputs_enc = tf.keras.layers.Input((n_enc_seq,))  # (bs, n_enc_seq)\n",
    "    inputs_dec = tf.keras.layers.Input((n_dec_seq,))  # (bs, n_dec_seq)\n",
    "    ################################################\n",
    "    # 입력 단어를 vector로 변환\n",
    "    embedding = tf.keras.layers.Embedding(n_vocab, d_model)\n",
    "    hidden_enc = embedding(inputs_enc)  # (bs, n_enc_seq, d_model)\n",
    "    hidden_dec = embedding(inputs_dec)  # (bs, n_dec_seq, d_model)\n",
    "\n",
    "    # Encoder LSTM (uni-direction, bi-direction 가능)\n",
    "    fw_cell = tf.keras.layers.LSTM(units=d_model, return_state=True)\n",
    "    bw_cell = tf.keras.layers.LSTM(units=d_model, go_backwards=True, return_state=True)\n",
    "    lstm_enc = tf.keras.layers.Bidirectional(fw_cell, backward_layer=bw_cell)\n",
    "    hidden_enc, fw_h, fw_c, bw_h, bw_c = lstm_enc(hidden_enc)  # (bs, d_model * 2), (bs, d_model), (bs, d_model), (bs, d_model), (bs, d_model)\n",
    "\n",
    "    # Concatenate hidden states and cell states\n",
    "    s_h = K.concatenate([fw_h, bw_h], axis=-1)  # (bs, d_model * 2)\n",
    "    s_c = K.concatenate([fw_c, bw_c], axis=-1)  # (bs, d_model * 2)\n",
    "\n",
    "    # Decoder LSTM (uni-direction만 가능)\n",
    "    lstm_dec = tf.keras.layers.LSTM(units=d_model * 2, return_sequences=True)\n",
    "    hidden_dec = lstm_dec(hidden_dec, initial_state=[s_h, s_c])  # (bs, n_dec_seq, d_model)\n",
    "    \n",
    "    # 다음단어 예측\n",
    "    dense_out = tf.keras.layers.Dense(units=n_vocab, activation=tf.nn.softmax)\n",
    "    outputs = dense_out(hidden_dec)  # (bs, n_dec_seq, n_vocab)\n",
    "    ################################################\n",
    "    # 학습할 모델 선언\n",
    "    model = tf.keras.Model(inputs=(inputs_enc, inputs_dec), outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "model = build_model(len(id_to_word), 5, 4, 6)\n",
    "# 모델 내용 그래프 출력\n",
    "tf.keras.utils.plot_model(model, 'model.png', show_shapes=True)"
   ]
  },
  {
   "source": [
    "# 다운로드\n",
    "# https://github.com/songys/Chatbot_data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 다운로드 및 목록 확인\n",
    "!wget https://github.com/songys/Chatbot_data/raw/master/ChatbotData%20.csv\n",
    "os.listdir('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일명 변경 및 목록 확인\n",
    "!mv 'ChatbotData .csv' ChatbotData.csv\n",
    "os.listdir('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 이동\n",
    "shutil.move('ChatbotData.csv', os.path.join(songys_dir, 'ChatbotData.csv'))\n",
    "os.listdir(songys_dir)"
   ]
  },
  {
   "source": [
    "# Vocabulary & config\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab loading\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(os.path.join(data_dir, 'ko_32000.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(vocab)  # number of vocabulary\n",
    "n_enc_seq = 32  # number of sequence 1\n",
    "n_dec_seq = 40  # number of sequence 2\n",
    "d_model = 256  # dimension of model"
   ]
  },
  {
   "source": [
    "# 데이터 분석"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data load\n",
    "df_train = pd.read_csv(os.path.join(songys_dir, 'ChatbotData.csv'), delimiter=',')\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train null 데이터 제거\n",
    "df_train = df_train.dropna()\n",
    "df_train"
   ]
  },
  {
   "source": [
    "# 1. Char"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 1-1. Q"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char 길이 데이터\n",
    "char_len = df_train['Q'].astype(\"str\").apply(len)\n",
    "char_len.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프에 대한 이미지 사이즈 선언\n",
    "# figsize: (가로, 세로) 형태의 튜플로 입력\n",
    "plt.figure(figsize=(8, 4))\n",
    "# histogram 선언\n",
    "# bins: 히스토그램 값들에 대한 버켓 범위, \n",
    "# range: x축 값의 범위\n",
    "# facecolor: 그래프 색상\n",
    "# label: 그래프에 대한 라벨\n",
    "plt.hist(char_len, bins=60, range=[0, 60], facecolor='r', label='train')\n",
    "# 그래프 제목\n",
    "plt.title('Lengh of char')\n",
    "# 그래프 x 축 라벨\n",
    "plt.xlabel('Number of char')\n",
    "# 그래프 y 축 라벨\n",
    "plt.ylabel('Count of review')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 길이\n",
    "print(f\"char 길이 최대:    {np.max(char_len):4d}\")\n",
    "print(f\"char 길이 최소:    {np.min(char_len):4d}\")\n",
    "print(f\"char 길이 평균:    {np.mean(char_len):7.2f}\")\n",
    "print(f\"char 길이 표준편차: {np.std(char_len):7.2f}\")\n",
    "# https://ko.wikipedia.org/wiki/%EB%B0%B1%EB%B6%84%EC%9C%84%EC%88%98\n",
    "# 백분위수(Percentile)는 크기가 있는 값들로 이뤄진 자료를 순서대로 나열했을 때 백분율로 나타낸 특정 위치의 값을 이르는 용어이다.\n",
    "# 일반적으로 크기가 작은 것부터 나열하여 가장 작은 것을 0, 가장 큰 것을 100으로 한다.\n",
    "# 100개의 값을 가진 어떤 자료의 20 백분위수는 그 자료의 값들 중 20번째로 작은 값을 뜻한다. 50 백분위수는 중앙값과 같다.\n",
    "percentile25 = np.percentile(char_len, 25)\n",
    "percentile50 = np.percentile(char_len, 50)\n",
    "percentile75 = np.percentile(char_len, 75)\n",
    "percentileIQR = percentile75 - percentile25\n",
    "percentileMAX = percentile75 + percentileIQR * 1.5\n",
    "print(f\"char 25/100분위:  {percentile25:7.2f}\")\n",
    "print(f\"char 50/100분위:  {percentile50:7.2f}\")\n",
    "print(f\"char 75/100분위:  {percentile75:7.2f}\")\n",
    "print(f\"char IQR:        {percentileIQR:7.2f}\")\n",
    "print(f\"char MAX/100분위: {percentileMAX:7.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 6))\n",
    "# 박스플롯 생성\n",
    "# 첫번째 파라메터: 여러 분포에 대한 데이터 리스트를\n",
    "# labels: 입력한 데이터에 대한 라벨\n",
    "# showmeans: 평균값을 표현\n",
    "# 참고: https://leebaro.tistory.com/entry/%EB%B0%95%EC%8A%A4-%ED%94%8C%EB%A1%AFbox-plot-%EC%84%A4%EB%AA%85\n",
    "plt.boxplot(char_len, labels=['char counts'], showmeans=True)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "# 1-2. A"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char 길이 데이터\n",
    "char_len = df_train['A'].astype(\"str\").apply(len)\n",
    "char_len.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프에 대한 이미지 사이즈 선언\n",
    "# figsize: (가로, 세로) 형태의 튜플로 입력\n",
    "plt.figure(figsize=(8, 4))\n",
    "# histogram 선언\n",
    "# bins: 히스토그램 값들에 대한 버켓 범위, \n",
    "# range: x축 값의 범위\n",
    "# facecolor: 그래프 색상\n",
    "# label: 그래프에 대한 라벨\n",
    "plt.hist(char_len, bins=80, range=[0, 80], facecolor='r', label='train')\n",
    "# 그래프 제목\n",
    "plt.title('Lengh of char')\n",
    "# 그래프 x 축 라벨\n",
    "plt.xlabel('Number of char')\n",
    "# 그래프 y 축 라벨\n",
    "plt.ylabel('Count of review')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 길이\n",
    "print(f\"char 길이 최대:    {np.max(char_len):4d}\")\n",
    "print(f\"char 길이 최소:    {np.min(char_len):4d}\")\n",
    "print(f\"char 길이 평균:    {np.mean(char_len):7.2f}\")\n",
    "print(f\"char 길이 표준편차: {np.std(char_len):7.2f}\")\n",
    "# https://ko.wikipedia.org/wiki/%EB%B0%B1%EB%B6%84%EC%9C%84%EC%88%98\n",
    "# 백분위수(Percentile)는 크기가 있는 값들로 이뤄진 자료를 순서대로 나열했을 때 백분율로 나타낸 특정 위치의 값을 이르는 용어이다.\n",
    "# 일반적으로 크기가 작은 것부터 나열하여 가장 작은 것을 0, 가장 큰 것을 100으로 한다.\n",
    "# 100개의 값을 가진 어떤 자료의 20 백분위수는 그 자료의 값들 중 20번째로 작은 값을 뜻한다. 50 백분위수는 중앙값과 같다.\n",
    "percentile25 = np.percentile(char_len, 25)\n",
    "percentile50 = np.percentile(char_len, 50)\n",
    "percentile75 = np.percentile(char_len, 75)\n",
    "percentileIQR = percentile75 - percentile25\n",
    "percentileMAX = percentile75 + percentileIQR * 1.5\n",
    "print(f\"char 25/100분위:  {percentile25:7.2f}\")\n",
    "print(f\"char 50/100분위:  {percentile50:7.2f}\")\n",
    "print(f\"char 75/100분위:  {percentile75:7.2f}\")\n",
    "print(f\"char IQR:        {percentileIQR:7.2f}\")\n",
    "print(f\"char MAX/100분위: {percentileMAX:7.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 6))\n",
    "# 박스플롯 생성\n",
    "# 첫번째 파라메터: 여러 분포에 대한 데이터 리스트를\n",
    "# labels: 입력한 데이터에 대한 라벨\n",
    "# showmeans: 평균값을 표현\n",
    "# 참고: https://leebaro.tistory.com/entry/%EB%B0%95%EC%8A%A4-%ED%94%8C%EB%A1%AFbox-plot-%EC%84%A4%EB%AA%85\n",
    "plt.boxplot(char_len, labels=['char counts'], showmeans=True)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "# 2. Word"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 2-1. Q"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word 길이 데이터\n",
    "word_len = df_train['Q'].astype(str).apply(lambda x:len(x.split(' ')))\n",
    "word_len.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프에 대한 이미지 사이즈 선언\n",
    "# figsize: (가로, 세로) 형태의 튜플로 입력\n",
    "plt.figure(figsize=(8, 4))\n",
    "# histogram 선언\n",
    "# bins: 히스토그램 값들에 대한 버켓 범위, \n",
    "# range: x축 값의 범위\n",
    "# facecolor: 그래프 색상\n",
    "# label: 그래프에 대한 라벨\n",
    "plt.hist(word_len, bins=15, range=[0, 15], facecolor='r', label='train')\n",
    "# 그래프 제목\n",
    "plt.title('Lengh of word')\n",
    "# 그래프 x 축 라벨\n",
    "plt.xlabel('Number of word')\n",
    "# 그래프 y 축 라벨\n",
    "plt.ylabel('Count of review')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 길이\n",
    "print(f\"word 길이 최대:    {np.max(word_len):4d}\")\n",
    "print(f\"word 길이 최소:    {np.min(word_len):4d}\")\n",
    "print(f\"word 길이 평균:    {np.mean(word_len):7.2f}\")\n",
    "print(f\"word 길이 표준편차: {np.std(word_len):7.2f}\")\n",
    "# https://ko.wikipedia.org/wiki/%EB%B0%B1%EB%B6%84%EC%9C%84%EC%88%98\n",
    "# 백분위수(Percentile)는 크기가 있는 값들로 이뤄진 자료를 순서대로 나열했을 때 백분율로 나타낸 특정 위치의 값을 이르는 용어이다.\n",
    "# 일반적으로 크기가 작은 것부터 나열하여 가장 작은 것을 0, 가장 큰 것을 100으로 한다.\n",
    "# 100개의 값을 가진 어떤 자료의 20 백분위수는 그 자료의 값들 중 20번째로 작은 값을 뜻한다. 50 백분위수는 중앙값과 같다.\n",
    "percentile25 = np.percentile(word_len, 25)\n",
    "percentile50 = np.percentile(word_len, 50)\n",
    "percentile75 = np.percentile(word_len, 75)\n",
    "percentileIQR = percentile75 - percentile25\n",
    "percentileMAX = percentile75 + percentileIQR * 1.5\n",
    "print(f\"word 25/100분위:  {percentile25:7.2f}\")\n",
    "print(f\"word 50/100분위:  {percentile50:7.2f}\")\n",
    "print(f\"word 75/100분위:  {percentile75:7.2f}\")\n",
    "print(f\"word IQR:        {percentileIQR:7.2f}\")\n",
    "print(f\"word MAX/100분위: {percentileMAX:7.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 6))\n",
    "# 박스플롯 생성\n",
    "# 첫번째 파라메터: 여러 분포에 대한 데이터 리스트를\n",
    "# labels: 입력한 데이터에 대한 라벨\n",
    "# showmeans: 평균값을 표현\n",
    "# 참고: https://leebaro.tistory.com/entry/%EB%B0%95%EC%8A%A4-%ED%94%8C%EB%A1%AFbox-plot-%EC%84%A4%EB%AA%85\n",
    "plt.boxplot(word_len, labels=['word counts'], showmeans=True)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "# 2-2. A"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word 길이 데이터\n",
    "word_len = df_train['A'].astype(str).apply(lambda x:len(x.split(' ')))\n",
    "word_len.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프에 대한 이미지 사이즈 선언\n",
    "# figsize: (가로, 세로) 형태의 튜플로 입력\n",
    "plt.figure(figsize=(8, 4))\n",
    "# histogram 선언\n",
    "# bins: 히스토그램 값들에 대한 버켓 범위, \n",
    "# range: x축 값의 범위\n",
    "# facecolor: 그래프 색상\n",
    "# label: 그래프에 대한 라벨\n",
    "plt.hist(word_len, bins=21, range=[0, 21], facecolor='r', label='train')\n",
    "# 그래프 제목\n",
    "plt.title('Lengh of word')\n",
    "# 그래프 x 축 라벨\n",
    "plt.xlabel('Number of word')\n",
    "# 그래프 y 축 라벨\n",
    "plt.ylabel('Count of review')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 길이\n",
    "print(f\"word 길이 최대:    {np.max(word_len):4d}\")\n",
    "print(f\"word 길이 최소:    {np.min(word_len):4d}\")\n",
    "print(f\"word 길이 평균:    {np.mean(word_len):7.2f}\")\n",
    "print(f\"word 길이 표준편차: {np.std(word_len):7.2f}\")\n",
    "# https://ko.wikipedia.org/wiki/%EB%B0%B1%EB%B6%84%EC%9C%84%EC%88%98\n",
    "# 백분위수(Percentile)는 크기가 있는 값들로 이뤄진 자료를 순서대로 나열했을 때 백분율로 나타낸 특정 위치의 값을 이르는 용어이다.\n",
    "# 일반적으로 크기가 작은 것부터 나열하여 가장 작은 것을 0, 가장 큰 것을 100으로 한다.\n",
    "# 100개의 값을 가진 어떤 자료의 20 백분위수는 그 자료의 값들 중 20번째로 작은 값을 뜻한다. 50 백분위수는 중앙값과 같다.\n",
    "percentile25 = np.percentile(word_len, 25)\n",
    "percentile50 = np.percentile(word_len, 50)\n",
    "percentile75 = np.percentile(word_len, 75)\n",
    "percentileIQR = percentile75 - percentile25\n",
    "percentileMAX = percentile75 + percentileIQR * 1.5\n",
    "print(f\"word 25/100분위:  {percentile25:7.2f}\")\n",
    "print(f\"word 50/100분위:  {percentile50:7.2f}\")\n",
    "print(f\"word 75/100분위:  {percentile75:7.2f}\")\n",
    "print(f\"word IQR:        {percentileIQR:7.2f}\")\n",
    "print(f\"word MAX/100분위: {percentileMAX:7.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 6))\n",
    "# 박스플롯 생성\n",
    "# 첫번째 파라메터: 여러 분포에 대한 데이터 리스트를\n",
    "# labels: 입력한 데이터에 대한 라벨\n",
    "# showmeans: 평균값을 표현\n",
    "# 참고: https://leebaro.tistory.com/entry/%EB%B0%95%EC%8A%A4-%ED%94%8C%EB%A1%AFbox-plot-%EC%84%A4%EB%AA%85\n",
    "plt.boxplot(word_len, labels=['word counts'], showmeans=True)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "# 3. Sentencepiece"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 3-1. Q"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word 길이 데이터\n",
    "spm_len = df_train['Q'].astype(str).apply(lambda x:len(vocab.encode_as_pieces(x)))\n",
    "spm_len.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프에 대한 이미지 사이즈 선언\n",
    "# figsize: (가로, 세로) 형태의 튜플로 입력\n",
    "plt.figure(figsize=(8, 4))\n",
    "# histogram 선언\n",
    "# bins: 히스토그램 값들에 대한 버켓 범위, \n",
    "# range: x축 값의 범위\n",
    "# facecolor: 그래프 색상\n",
    "# label: 그래프에 대한 라벨\n",
    "plt.hist(spm_len, bins=26, range=[0, 26], facecolor='r', label='train')\n",
    "# 그래프 제목\n",
    "plt.title('Lengh of spm')\n",
    "# 그래프 x 축 라벨\n",
    "plt.xlabel('Number of spm')\n",
    "# 그래프 y 축 라벨\n",
    "plt.ylabel('Count of review')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 길이\n",
    "print(f\"spm 길이 최대:    {np.max(spm_len):4d}\")\n",
    "print(f\"spm 길이 최소:    {np.min(spm_len):4d}\")\n",
    "print(f\"spm 길이 평균:    {np.mean(spm_len):7.2f}\")\n",
    "print(f\"spm 길이 표준편차: {np.std(spm_len):7.2f}\")\n",
    "# https://ko.wikipedia.org/wiki/%EB%B0%B1%EB%B6%84%EC%9C%84%EC%88%98\n",
    "# 백분위수(Percentile)는 크기가 있는 값들로 이뤄진 자료를 순서대로 나열했을 때 백분율로 나타낸 특정 위치의 값을 이르는 용어이다.\n",
    "# 일반적으로 크기가 작은 것부터 나열하여 가장 작은 것을 0, 가장 큰 것을 100으로 한다.\n",
    "# 100개의 값을 가진 어떤 자료의 20 백분위수는 그 자료의 값들 중 20번째로 작은 값을 뜻한다. 50 백분위수는 중앙값과 같다.\n",
    "percentile25 = np.percentile(spm_len, 25)\n",
    "percentile50 = np.percentile(spm_len, 50)\n",
    "percentile75 = np.percentile(spm_len, 75)\n",
    "percentileIQR = percentile75 - percentile25\n",
    "percentileMAX = percentile75 + percentileIQR * 1.5\n",
    "print(f\"spm 25/100분위:  {percentile25:7.2f}\")\n",
    "print(f\"spm 50/100분위:  {percentile50:7.2f}\")\n",
    "print(f\"spm 75/100분위:  {percentile75:7.2f}\")\n",
    "print(f\"spm IQR:        {percentileIQR:7.2f}\")\n",
    "print(f\"spm MAX/100분위: {percentileMAX:7.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 6))\n",
    "# 박스플롯 생성\n",
    "# 첫번째 파라메터: 여러 분포에 대한 데이터 리스트를\n",
    "# labels: 입력한 데이터에 대한 라벨\n",
    "# showmeans: 평균값을 표현\n",
    "# 참고: https://leebaro.tistory.com/entry/%EB%B0%95%EC%8A%A4-%ED%94%8C%EB%A1%AFbox-plot-%EC%84%A4%EB%AA%85\n",
    "plt.boxplot(spm_len, labels=['spm counts'], showmeans=True)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "# 3-2. A"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word 길이 데이터\n",
    "spm_len = df_train['A'].astype(str).apply(lambda x:len(vocab.encode_as_pieces(x)))\n",
    "spm_len.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프에 대한 이미지 사이즈 선언\n",
    "# figsize: (가로, 세로) 형태의 튜플로 입력\n",
    "plt.figure(figsize=(8, 4))\n",
    "# histogram 선언\n",
    "# bins: 히스토그램 값들에 대한 버켓 범위, \n",
    "# range: x축 값의 범위\n",
    "# facecolor: 그래프 색상\n",
    "# label: 그래프에 대한 라벨\n",
    "plt.hist(spm_len, bins=35, range=[0, 35], facecolor='r', label='train')\n",
    "# 그래프 제목\n",
    "plt.title('Lengh of spm')\n",
    "# 그래프 x 축 라벨\n",
    "plt.xlabel('Number of spm')\n",
    "# 그래프 y 축 라벨\n",
    "plt.ylabel('Count of review')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 길이\n",
    "print(f\"spm 길이 최대:    {np.max(spm_len):4d}\")\n",
    "print(f\"spm 길이 최소:    {np.min(spm_len):4d}\")\n",
    "print(f\"spm 길이 평균:    {np.mean(spm_len):7.2f}\")\n",
    "print(f\"spm 길이 표준편차: {np.std(spm_len):7.2f}\")\n",
    "# https://ko.wikipedia.org/wiki/%EB%B0%B1%EB%B6%84%EC%9C%84%EC%88%98\n",
    "# 백분위수(Percentile)는 크기가 있는 값들로 이뤄진 자료를 순서대로 나열했을 때 백분율로 나타낸 특정 위치의 값을 이르는 용어이다.\n",
    "# 일반적으로 크기가 작은 것부터 나열하여 가장 작은 것을 0, 가장 큰 것을 100으로 한다.\n",
    "# 100개의 값을 가진 어떤 자료의 20 백분위수는 그 자료의 값들 중 20번째로 작은 값을 뜻한다. 50 백분위수는 중앙값과 같다.\n",
    "percentile25 = np.percentile(spm_len, 25)\n",
    "percentile50 = np.percentile(spm_len, 50)\n",
    "percentile75 = np.percentile(spm_len, 75)\n",
    "percentileIQR = percentile75 - percentile25\n",
    "percentileMAX = percentile75 + percentileIQR * 1.5\n",
    "print(f\"spm 25/100분위:  {percentile25:7.2f}\")\n",
    "print(f\"spm 50/100분위:  {percentile50:7.2f}\")\n",
    "print(f\"spm 75/100분위:  {percentile75:7.2f}\")\n",
    "print(f\"spm IQR:        {percentileIQR:7.2f}\")\n",
    "print(f\"spm MAX/100분위: {percentileMAX:7.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 6))\n",
    "# 박스플롯 생성\n",
    "# 첫번째 파라메터: 여러 분포에 대한 데이터 리스트를\n",
    "# labels: 입력한 데이터에 대한 라벨\n",
    "# showmeans: 평균값을 표현\n",
    "# 참고: https://leebaro.tistory.com/entry/%EB%B0%95%EC%8A%A4-%ED%94%8C%EB%A1%AFbox-plot-%EC%84%A4%EB%AA%85\n",
    "plt.boxplot(spm_len, labels=['spm counts'], showmeans=True)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "# 4. Word Cloud"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 합치기\n",
    "train_set = pd.Series(df_train['Q'].tolist() + df_train['A'].tolist()).astype(str)\n",
    "train_set.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train review documents\n",
    "train_review = [review for review in train_set if type(review) is str]\n",
    "train_review[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(train_review[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud\n",
    "# window: C:/Windows/Fonts/malgun.ttf, mac: /Library/Fonts/AppleGothic.ttf, colab: /usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf\n",
    "wordcloud = WordCloud(width=800, height=800, font_path='/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf').generate(' '.join(train_review))\n",
    "plt.figure(figsize=(10, 10))\n",
    "# https://datascienceschool.net/view-notebook/6e71dbff254542d9b0a054a7c98b34ec/\n",
    "# image 출력, interpolation 이미지 시각화 옵션\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "# Loss & Acc"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    pad 부분을 제외하고 loss를 계산하는 함수\n",
    "    :param y_true: 정답\n",
    "    :param y_pred: 예측 값\n",
    "    :retrun loss: pad 부분이 제외된 loss 값\n",
    "    \"\"\"\n",
    "    # loss = sparse_entropy = tf.keras.losses.SparseCategoricalCrossentropy()(y_true, y_pred)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)(y_true, y_pred)\n",
    "    mask = tf.not_equal(y_true, 0)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    # print(mask)\n",
    "    loss *= mask\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    pad 부분을 제외하고 accuracy를 계산하는 함수\n",
    "    :param y_true: 정답\n",
    "    :param y_pred: 예측 값\n",
    "    :retrun loss: pad 부분이 제외된 accuracy 값\n",
    "    \"\"\"\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    # print(y_true)\n",
    "    y_pred_class = tf.cast(tf.argmax(y_pred, axis=-1), tf.float32)\n",
    "    # print(y_pred_class)\n",
    "    matches = tf.cast(tf.equal(y_true, y_pred_class), tf.float32)\n",
    "    # print(matches)\n",
    "    mask = tf.not_equal(y_true, 0)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    # print(mask)\n",
    "    matches *= mask\n",
    "    # print(matches)\n",
    "    # accuracy = tf.reduce_sum(matches) / tf.maximum(tf.reduce_sum(tf.ones_like(matches)), 1)\n",
    "    accuracy = tf.reduce_sum(matches) / tf.maximum(tf.reduce_sum(mask), 1)\n",
    "    return accuracy"
   ]
  },
  {
   "source": [
    "# Sample Data Project"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 1. Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(os.path.join(songys_dir, 'ChatbotData.csv'))\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.dropna()\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤하게 10개만 확인\n",
    "df_train = df_train.sample(10)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(df, n_enc_seq, n_dec_seq):\n",
    "    \"\"\"\n",
    "    Quora 학습 데이터 생성\n",
    "    :param df: data frame\n",
    "    :param n_enc_seq: number of encoder sequence\n",
    "    :param n_dec_seq: number of decoder sequence\n",
    "    :return enc_inputs: encoder input data\n",
    "    :return dec_inputs: decoder input data\n",
    "    :return labels: label data\n",
    "    \"\"\"\n",
    "    n_enc_max = n_enc_seq  # no addtional tag\n",
    "    n_dec_max = n_dec_seq - 1  # [BOS] or [EOS]\n",
    "    enc_inputs = np.zeros((len(df), n_enc_seq)).astype(np.int32)\n",
    "    dec_inputs = np.zeros((len(df), n_dec_seq)).astype(np.int32)\n",
    "    labels = np.zeros((len(df), n_dec_seq)).astype(np.int32)\n",
    "    index = 0\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        # tokens 저장\n",
    "        print()\n",
    "        ###########################################\n",
    "        Q = row['Q']\n",
    "        A = row['A']\n",
    "        print(Q, '/', A)\n",
    "\n",
    "        tokens_q = vocab.encode_as_pieces(Q)\n",
    "        print(len(tokens_q), ':', tokens_q)\n",
    "        tokens_a = vocab.encode_as_pieces(A)\n",
    "        print(len(tokens_a), ':', tokens_a)\n",
    "        # token id 생성\n",
    "        tokens_ids_q = vocab.encode_as_ids(Q)[:n_enc_max]\n",
    "        print(len(tokens_ids_q), ':', tokens_ids_q)\n",
    "        tokens_ids_a = vocab.encode_as_ids(A)[:n_dec_max]\n",
    "        print(len(tokens_ids_a), ':', tokens_ids_a)\n",
    "        # decoder 입력 출력 생성\n",
    "        tokens_dec_in = [vocab.bos_id()] + tokens_ids_a\n",
    "        tokens_dec_out = tokens_ids_a + [vocab.eos_id()]\n",
    "        # encoder 입력, dec 입력, dec 출력에 pad 추가\n",
    "        tokens_ids_q += [0] * (n_enc_seq - len(tokens_ids_q))\n",
    "        print(len(tokens_ids_q), ':', tokens_ids_q)\n",
    "        tokens_dec_in += [0] * (n_dec_seq - len(tokens_dec_in))\n",
    "        print(len(tokens_dec_in), ':', tokens_dec_in)\n",
    "        tokens_dec_out += [0] * (n_dec_seq - len(tokens_dec_out))\n",
    "        print(len(tokens_dec_out), ':', tokens_dec_out)\n",
    "        # 값 저장\n",
    "        enc_inputs[index] = tokens_ids_q\n",
    "        dec_inputs[index] = tokens_dec_in\n",
    "        labels[index] = tokens_dec_out\n",
    "        index += 1\n",
    "        ###########################################\n",
    "    return enc_inputs, dec_inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data 생성\n",
    "train_enc_inputs, train_dec_inputs, train_labels = load_data(df_train, n_enc_seq, n_dec_seq)\n",
    "train_enc_inputs, train_dec_inputs, train_labels"
   ]
  },
  {
   "source": [
    "# 2. 학습"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "model = build_model(len(vocab), d_model, n_enc_seq, n_dec_seq)\n",
    "# 모델 내용 그래프 출력\n",
    "tf.keras.utils.plot_model(model, 'model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 loss, optimizer, metric 정의\n",
    "model.compile(loss=lm_loss, optimizer='adam', metrics=[lm_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='lm_acc', patience=100)\n",
    "# save weights callback\n",
    "save_weights = tf.keras.callbacks.ModelCheckpoint(os.path.join(songys_dir, 'lstm.hdf5'),\n",
    "                                                  monitor='lm_acc',\n",
    "                                                  verbose=1,\n",
    "                                                  save_best_only=True,\n",
    "                                                  mode=\"max\",\n",
    "                                                  save_freq=\"epoch\",\n",
    "                                                  save_weights_only=True)\n",
    "# csv logger\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(os.path.join(songys_dir, 'lstm.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "history = model.fit((train_enc_inputs, train_dec_inputs),\n",
    "                    train_labels,\n",
    "                    epochs=400,\n",
    "                    batch_size=256,\n",
    "                    callbacks=[early_stopping, save_weights, csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], 'b-', label='loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['lm_acc'], 'g-', label='acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "# 3. Inference"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "model = build_model(len(vocab), d_model, n_enc_seq, n_dec_seq)\n",
    "# train weight로 초기화\n",
    "model.load_weights(os.path.join(songys_dir, 'lstm.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_chat(vocab, model, n_enc_seq, n_dec_seq, string):\n",
    "    \"\"\"\n",
    "    seq2seq chat\n",
    "    :param vocab: vocab\n",
    "    :param model: model\n",
    "    :param n_enc_seq: number of enc seqence\n",
    "    :param n_dec_seq: number of dec seqence\n",
    "    :param string: inpust string\n",
    "    \"\"\"\n",
    "    #####################################\n",
    "    # qeustion\n",
    "    q = vocab.encode_as_pieces(string)\n",
    "    # print(q)\n",
    "    q_id = vocab.encode_as_ids(string)[:n_enc_seq]\n",
    "    q_id += [0] * (n_enc_seq - len(q_id))\n",
    "    # print(len(q_id), q_id)\n",
    "    assert len(q_id) == n_enc_seq\n",
    "\n",
    "    # answer\n",
    "    a_id = [vocab.bos_id()]\n",
    "    a_id += [0] * (n_dec_seq - len(a_id))\n",
    "    # print(len(a_id), a_id)\n",
    "    assert len(a_id) == n_dec_seq\n",
    "\n",
    "    # 처음부터 예측\n",
    "    start_idx = 0\n",
    "\n",
    "    for _ in range(0, n_dec_seq - 1):\n",
    "        # 다음단어 예측\n",
    "        outputs = model.predict((np.array([q_id]), np.array([a_id])))\n",
    "        # print(outputs.shape)\n",
    "        prob = outputs[0][start_idx]\n",
    "        word_id = int(np.argmax(prob))\n",
    "        # print(prob[word_id - 5:word_id + 5])\n",
    "        # print(word_id, vocab.id_to_piece(word_id))\n",
    "        # eos면 종료\n",
    "        if word_id == vocab.eos_id():\n",
    "            break\n",
    "        a_id[start_idx + 1] = word_id\n",
    "        start_idx += 1\n",
    "        # print(start_idx, a_id)\n",
    "        # print()\n",
    "    # 예측된 문장 생성\n",
    "    predict_id = a_id[1:start_idx + 1]\n",
    "    # print(predict_id)\n",
    "    predict_str = vocab.decode_ids(predict_id)\n",
    "    return predict_str\n",
    "    #####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = '너는 누구니'\n",
    "do_chat(vocab, model, n_enc_seq, n_dec_seq, string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    string = input('질문 > ')\n",
    "    string = string.strip()\n",
    "    if len(string) == 0:\n",
    "        break\n",
    "    predict_str = do_chat(vocab, model, n_enc_seq, n_dec_seq, string)\n",
    "    print(f'답변 > {predict_str}')"
   ]
  },
  {
   "source": [
    "# All Data Project"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 1. Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(os.path.join(songys_dir, 'ChatbotData.csv'))\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.dropna()\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(df, n_enc_seq, n_dec_seq):\n",
    "    \"\"\"\n",
    "    Quora 학습 데이터 생성\n",
    "    :param df: data frame\n",
    "    :param n_enc_seq: number of encoder sequence\n",
    "    :param n_dec_seq: number of decoder sequence\n",
    "    :return enc_inputs: encoder input data\n",
    "    :return dec_inputs: decoder input data\n",
    "    :return labels: label data\n",
    "    \"\"\"\n",
    "    n_enc_max = n_enc_seq\n",
    "    n_dec_max = n_dec_seq - 1\n",
    "    enc_inputs = np.zeros((len(df), n_enc_seq)).astype(np.int32)\n",
    "    dec_inputs = np.zeros((len(df), n_dec_seq)).astype(np.int32)\n",
    "    labels = np.zeros((len(df), n_dec_seq)).astype(np.int32)\n",
    "    index = 0\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        # tokens 저장\n",
    "        # print()\n",
    "        Q = row['Q']\n",
    "        A = row['A']\n",
    "        # print(Q, '/', A)\n",
    "\n",
    "        tokens_q = vocab.encode_as_pieces(Q)\n",
    "        # print(len(tokens_q), ':', tokens_q)\n",
    "        tokens_a = vocab.encode_as_pieces(A)\n",
    "        # print(len(tokens_a), ':', tokens_a)\n",
    "\n",
    "        tokens_ids_q = vocab.encode_as_ids(Q)\n",
    "        # print(len(tokens_ids_q), ':', tokens_ids_q)\n",
    "        tokens_ids_a = vocab.encode_as_ids(A)\n",
    "        # print(len(tokens_ids_a), ':', tokens_ids_a)\n",
    "\n",
    "        tokens_ids_q = tokens_ids_q[:n_enc_max]\n",
    "        # print(len(tokens_ids_q), ':', tokens_ids_q)\n",
    "        tokens_ids_a = tokens_ids_a[:n_dec_max]\n",
    "        # print(len(tokens_ids_a), ':', tokens_ids_a)\n",
    "\n",
    "        tokens_dec_in = [vocab.bos_id()] + tokens_ids_a\n",
    "        tokens_dec_out = tokens_ids_a + [vocab.eos_id()]\n",
    "\n",
    "        tokens_ids_q += [0] * (n_enc_seq - len(tokens_ids_q))\n",
    "        # print(len(tokens_ids_q), ':', tokens_ids_q)\n",
    "        tokens_dec_in += [0] * (n_dec_seq - len(tokens_dec_in))\n",
    "        # print(len(tokens_dec_in), ':', tokens_dec_in)\n",
    "        tokens_dec_out += [0] * (n_dec_seq - len(tokens_dec_out))\n",
    "        # print(len(tokens_dec_out), ':', tokens_dec_out)\n",
    "\n",
    "        enc_inputs[index] = tokens_ids_q\n",
    "        dec_inputs[index] = tokens_dec_in\n",
    "        labels[index] = tokens_dec_out\n",
    "        index += 1\n",
    "    return enc_inputs, dec_inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data 생성\n",
    "train_enc_inputs, train_dec_inputs, train_labels = load_data(df_train, n_enc_seq, n_dec_seq)\n",
    "train_enc_inputs, train_dec_inputs, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_enc_inputs.shape, train_dec_inputs.shape, train_labels.shape"
   ]
  },
  {
   "source": [
    "# 2. 학습"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "model = build_model(len(vocab), d_model, n_enc_seq, n_dec_seq)\n",
    "# 모델 내용 그래프 출력\n",
    "tf.keras.utils.plot_model(model, 'model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 loss, optimizer, metric 정의\n",
    "model.compile(loss=lm_loss, optimizer='adam', metrics=[lm_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='lm_acc', patience=10)\n",
    "# save weights callback\n",
    "save_weights = tf.keras.callbacks.ModelCheckpoint(os.path.join(songys_dir, 'lstm.hdf5'),\n",
    "                                                  monitor='lm_acc',\n",
    "                                                  verbose=1,\n",
    "                                                  save_best_only=True,\n",
    "                                                  mode=\"max\",\n",
    "                                                  save_freq=\"epoch\",\n",
    "                                                  save_weights_only=True)\n",
    "# csv logger\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(os.path.join(songys_dir, 'lstm.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "history = model.fit((train_enc_inputs, train_dec_inputs),\n",
    "                    train_labels,\n",
    "                    epochs=100,\n",
    "                    batch_size=256,\n",
    "                    callbacks=[early_stopping, save_weights, csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], 'b-', label='loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['lm_acc'], 'g-', label='acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "# 3. Inference"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "model = build_model(len(vocab), d_model, n_enc_seq, n_dec_seq)\n",
    "# train weight로 초기화\n",
    "model.load_weights(os.path.join(songys_dir, 'lstm.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_chat(vocab, model, n_enc_seq, n_dec_seq, string):\n",
    "    \"\"\"\n",
    "    seq2seq chat\n",
    "    :param vocab: vocab\n",
    "    :param model: model\n",
    "    :param n_enc_seq: number of enc seqence\n",
    "    :param n_dec_seq: number of dec seqence\n",
    "    :param string: inpust string\n",
    "    \"\"\"\n",
    "    # qeustion\n",
    "    q = vocab.encode_as_pieces(string)\n",
    "    q_id = [vocab.piece_to_id(p) for p in q][:n_enc_seq]\n",
    "    q_id += [0] * (n_enc_seq - len(q_id))\n",
    "    assert len(q_id) == n_enc_seq\n",
    "\n",
    "    # answer\n",
    "    a_id = [vocab.bos_id()]\n",
    "    a_id += [0] * (n_dec_seq - len(a_id))\n",
    "    assert len(a_id) == n_dec_seq\n",
    "\n",
    "    # 처음부터 예측\n",
    "    start_idx = 0\n",
    "\n",
    "    for _ in range(start_idx, n_dec_seq - 1):\n",
    "        outputs = model.predict((np.array([q_id]), np.array([a_id])))\n",
    "        prob = outputs[0][start_idx]\n",
    "        word_id = np.argmax(prob)\n",
    "        if word_id == vocab.eos_id():\n",
    "            break\n",
    "        a_id[start_idx + 1] = int(word_id)\n",
    "        start_idx += 1\n",
    "    predict_id = a_id[1:start_idx + 1]\n",
    "    predict_str = vocab.decode_ids(predict_id)\n",
    "    return predict_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    string = input('질문 > ')\n",
    "    string = string.strip()\n",
    "    if len(string) == 0:\n",
    "        break\n",
    "    predict_str = do_chat(vocab, model, n_enc_seq, n_dec_seq, string)\n",
    "    print(f'답변 > {predict_str}')"
   ]
  }
 ]
}