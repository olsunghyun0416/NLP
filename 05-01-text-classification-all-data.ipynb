{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "0adcc2737ebf6a4a119f135174df96668767fca1ef1112612db5ecadf2b6d608"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 한글 폰트 설치"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "zsh:1: command not found: apt-get\n",
      "zsh:1: no matches found: fonts-nanum*\n"
     ]
    }
   ],
   "source": [
    "# 폰트 설치\n",
    "!apt-get update -qq # 나눔고딕 설치\n",
    "!apt-get install fonts-nanum* -qq\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# 폰트 로딩\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager._rebuild()"
   ]
  },
  {
   "source": [
    "# Install"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "source": [
    "# Evn"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import json\n",
    "import zipfile\n",
    "import math\n",
    "import copy\n",
    "import collections\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed initialize\n",
    "random_seed = 1234\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google drive mount\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data dir\n",
    "data_dir = '/content/drive/MyDrive/Data/nlp' # change /content/ ~ /nlp' to your drive file path\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nsmc dir\n",
    "nsmc_dir = os.path.join(data_dir, 'nsmc')\n",
    "if not os.path.exists(nsmc_dir):\n",
    "    os.makedirs(nsmc_dir)\n",
    "os.listdir(nsmc_dir)"
   ]
  },
  {
   "source": [
    "# Vocabulary & config"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab loading\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(os.path.join(data_dir, 'ko_32000.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(vocab)  # number of vocabulary\n",
    "n_seq = 47  # number of sequence\n",
    "d_model = 256  # dimension of model\n",
    "n_out = 2  # number of output class"
   ]
  },
  {
   "source": [
    "# 모델링"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 문장\n",
    "sentences = [\n",
    "    '나는 오늘 기분이 좋아',\n",
    "    '나는 오늘 우울해'\n",
    "]\n",
    "\n",
    "# 출력 정답\n",
    "labels = [1, 0]  # 긍정(1), 부정(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 문장을 띄어쓰기 단위로 분할\n",
    "words = []\n",
    "for sentence in sentences:\n",
    "    words.extend(sentence.split())\n",
    "\n",
    "# 중복 단어 제거\n",
    "words = list(dict.fromkeys(words))\n",
    "\n",
    "# 각 단어별 고유한 번호 부여\n",
    "word_to_id = {'[PAD]': 0, '[UNK]': 1}\n",
    "for word in words:\n",
    "    word_to_id[word] = len(word_to_id)\n",
    "\n",
    "# 각 숫자별 단어 부여\n",
    "id_to_word = {_id:word for word, _id in word_to_id.items()}\n",
    "\n",
    "word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습용 입력 데이터 생성\n",
    "train_inputs = []\n",
    "for sentence in sentences:\n",
    "    train_inputs.append([word_to_id[word] for word in sentence.split()])\n",
    "\n",
    "# train label\n",
    "train_labels = labels\n",
    "\n",
    "# 문장의 길이를 모두 동일하게 변경 (최대길이 4)\n",
    "for row in train_inputs:\n",
    "    row += [0] * (4 - len(row))\n",
    "\n",
    "# train inputs을 numpy array로 변환\n",
    "train_inputs = np.array(train_inputs)\n",
    "\n",
    "# 학습용 정답을 numpy array로 변환\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "train_inputs, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 단어를 vector로 변환\n",
    "embedding = tf.keras.layers.Embedding(len(word_to_id), 5)\n",
    "hidden = embedding(train_inputs)\n",
    "hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 단어 벡터의 최대값 기준으로 벡터를 더해서 차원을 줄여줌 (문장 vector 생성)\n",
    "pool = tf.keras.layers.GlobalMaxPool1D()\n",
    "hidden_pool = pool(hidden)\n",
    "hidden_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 vector를 이용해서 긍정(1), 부정(0) 확률값 예측\n",
    "linear = tf.keras.layers.Dense(2, activation=tf.nn.softmax)\n",
    "outputs = linear(hidden_pool)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측을 위한 wegiths\n",
    "weight, bias = linear.get_weights()\n",
    "weight, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_vocab, d_model, n_seq, n_out):\n",
    "    \"\"\"\n",
    "    문장의 감정분류 모델\n",
    "    :param n_vocab: vocabulary 단어 수\n",
    "    :param d_model: 단어를 의미하는 벡터의 차원 수\n",
    "    :param n_seq: 문장길이 (단어 수)\n",
    "    :param n_out: 예측할 class 개수\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.layers.Input((n_seq,))  # (bs, n_seq)\n",
    "    ######################################################\n",
    "    # 입력 단어를 vector로 변환\n",
    "    embedding = tf.keras.layers.Embedding(n_vocab, d_model)\n",
    "    hidden = embedding(inputs)  # (bs, n_seq, d_model)\n",
    "\n",
    "    # RNN, CNN, Dense\n",
    "\n",
    "    # 각 단어 벡터의 최대값 기준으로 벡터를 더해서 차원을 줄여줌 (문장 vector 생성)\n",
    "    pool = tf.keras.layers.GlobalMaxPool1D()\n",
    "    hidden_pool = pool(hidden)  # (bs, d_model)\n",
    "\n",
    "    # 문장 vector를 이용해서 긍정(1), 부정(0) 확률값 예측\n",
    "    linear = tf.keras.layers.Dense(n_out, activation=tf.nn.softmax)\n",
    "    outputs = linear(hidden_pool)  # (bs, n_out)\n",
    "    ######################################################\n",
    "    # 학습할 모델 선언\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "model = build_model(len(word_to_id), 8, 4, 2)\n",
    "# 모델 내용 그래프 출력\n",
    "tf.keras.utils.plot_model(model, 'model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(train_inputs)"
   ]
  },
  {
   "source": [
    "# 다운로드\n",
    "# https://github.com/e9t/nsmc"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/e9t/nsmc/raw/master/ratings_train.txt\n",
    "!wget https://github.com/e9t/nsmc/raw/master/ratings_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.move('ratings_train.txt', os.path.join(nsmc_dir, 'ratings_train.txt'))\n",
    "shutil.move('ratings_test.txt', os.path.join(nsmc_dir, 'ratings_test.txt'))\n",
    "os.listdir(nsmc_dir)"
   ]
  },
  {
   "source": [
    "# All Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data load\n",
    "df_train = pd.read_csv(os.path.join(nsmc_dir, 'ratings_train.txt'), delimiter='\\t')\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train null 데이터 제거\n",
    "df_train = df_train.dropna()\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data load\n",
    "df_test = pd.read_csv(os.path.join(nsmc_dir, 'ratings_test.txt'), delimiter='\\t')\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train null 데이터 제거\n",
    "df_test = df_test.dropna()\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(df, n_seq):\n",
    "    \"\"\"\n",
    "    NSMC 학습 데이터 생성\n",
    "    :param df: data frame\n",
    "    :param n_seq: number of sequence\n",
    "    :return inputs: input data\n",
    "    :return labels: label data\n",
    "    \"\"\"\n",
    "    inputs = np.zeros((len(df), n_seq)).astype(np.int32)\n",
    "    labels = np.zeros((len(df),))\n",
    "    index = 0\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        # tokens 저장\n",
    "        # print()\n",
    "        label = row['label']\n",
    "        document = row['document']\n",
    "        # print(label, document)\n",
    "        tokens = vocab.encode_as_pieces(document)\n",
    "        # print(len(tokens), ':', tokens)\n",
    "        token_ids = vocab.encode_as_ids(document)\n",
    "        # print(len(token_ids), ':', token_ids)\n",
    "        token_ids = token_ids[:n_seq]\n",
    "        # print(len(token_ids), ':', token_ids)\n",
    "        token_ids += [0] * (n_seq - len(token_ids))\n",
    "        # print(len(token_ids), ':', token_ids)\n",
    "\n",
    "        labels[index] = label\n",
    "        inputs[index] = token_ids\n",
    "        index += 1\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data 생성\n",
    "train_inputs, train_labels = load_data(df_train, n_seq)\n",
    "train_inputs, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data 생성\n",
    "test_inputs, test_labels = load_data(df_test, n_seq)\n",
    "test_inputs, test_labels"
   ]
  },
  {
   "source": [
    "# 학습"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "model = build_model(len(vocab), d_model, n_seq, n_out)\n",
    "# 모델 내용 그래프 출력\n",
    "tf.keras.utils.plot_model(model, 'model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 loss, optimizer, metric 정의\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)\n",
    "# save weights callback\n",
    "save_weights = tf.keras.callbacks.ModelCheckpoint(os.path.join(nsmc_dir, 'stub.hdf5'),\n",
    "                                                  monitor='val_accuracy',\n",
    "                                                  verbose=1,\n",
    "                                                  save_best_only=True,\n",
    "                                                  mode=\"max\",\n",
    "                                                  save_freq=\"epoch\",\n",
    "                                                  save_weights_only=True)\n",
    "# csv logger\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(os.path.join(nsmc_dir, 'stub.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "history = model.fit(train_inputs,\n",
    "                    train_labels,\n",
    "                    validation_data=(test_inputs, test_labels),\n",
    "                    epochs=100,\n",
    "                    batch_size=256,\n",
    "                    callbacks=[early_stopping, save_weights, csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], 'b-', label='loss')\n",
    "plt.plot(history.history['val_loss'], 'r--', label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], 'g-', label='acc')\n",
    "plt.plot(history.history['val_accuracy'], 'k--', label='val_acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "# Inference"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "model = build_model(n_vocab, d_model, n_seq, n_out)\n",
    "# train weight로 초기화\n",
    "model.load_weights(os.path.join(nsmc_dir, 'stub.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측\n",
    "logits = model.predict(test_inputs)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 확률 최대 값을 예측으로 사용\n",
    "test_preds = np.argmax(logits, axis=-1)\n",
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = np.zeros((n_out, n_out)).astype(np.int)\n",
    "for y_true, y_pred in zip(test_labels, test_preds):\n",
    "    confusion_matrix[int(y_true), int(y_pred)] += 1\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = confusion_matrix[1, 1]\n",
    "tn = confusion_matrix[0, 0]\n",
    "fp = confusion_matrix[0, 1]\n",
    "fn = confusion_matrix[1, 0]\n",
    "\n",
    "accuracy = (tp + tn) / max((tp + tn + fp + fn), 1)\n",
    "print(f'accuracy: {accuracy}')\n",
    "precision = (tp) / max((tp + fp), 1)\n",
    "print(f'precision: {precision}')\n",
    "recall = (tp) / max((tp + fn), 1)\n",
    "print(f'recall: {recall}')\n",
    "f1 = 2 * (precision * recall) / max((precision + recall), 1)\n",
    "print(f'f1: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_predict(model, n_seq, string):\n",
    "    \"\"\"\n",
    "    입력에 대한 답변 생성하는 함수\n",
    "    :param model: model\n",
    "    :param n_seq: 입력 개수\n",
    "    :param string: 입력 문자열\n",
    "    \"\"\"\n",
    "    # token 생성: <string tokens>, [PAD] tokens\n",
    "    token = vocab.encode_as_ids(string)[:n_seq]\n",
    "    token += [0] * (n_seq - len(token))\n",
    "    token = token[:n_seq]\n",
    "\n",
    "    y_pred = model.predict(np.array([token]))\n",
    "    y_pred_class = K.argmax(y_pred, axis=-1)\n",
    "\n",
    "    return \"긍정\" if y_pred_class[0] == 1 else \"부정\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    print(\"input > \", end=\"\")\n",
    "    string = str(input())\n",
    "    if len(string) == 0:\n",
    "        break\n",
    "    print(f\"output > {do_predict(model, n_seq, string)}\")"
   ]
  }
 ]
}