{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "0adcc2737ebf6a4a119f135174df96668767fca1ef1112612db5ecadf2b6d608"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 한글 폰트 설치"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "zsh:1: command not found: apt-get\n",
      "zsh:1: no matches found: fonts-nanum*\n"
     ]
    }
   ],
   "source": [
    "# 폰트 설치\n",
    "!apt-get update -qq # 나눔고딕 설치\n",
    "!apt-get install fonts-nanum* -qq\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# 폰트 로딩\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager._rebuild()"
   ]
  },
  {
   "source": [
    "# Install"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "source": [
    "# Evn"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import json\n",
    "import zipfile\n",
    "import math\n",
    "import copy\n",
    "import collections\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed initialize\n",
    "random_seed = 1234\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "source": [
    "# Word2Vec (Skip-gram)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 1. 모델링"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 문장\n",
    "sentences = \"\"\"나는 오늘 기분이 좋아\n",
    "나는 오늘 우울해\"\"\"\n",
    "\n",
    "# 문장을 띄어쓰기 단위로 분할\n",
    "words = sentences.split()\n",
    "\n",
    "# 중복 단어 제거\n",
    "words = list(dict.fromkeys(words))\n",
    "\n",
    "# 각 단어별 고유한 번호 부여\n",
    "word_to_id = {'[PAD]': 0, '[UNK]': 1}\n",
    "for word in words:\n",
    "    word_to_id[word] = len(word_to_id)\n",
    "word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력(c), 출력(o) 정의\n",
    "inputs = np.array([word_to_id['나는']])\n",
    "labels = np.array([word_to_id['기분이']])\n",
    "inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center word embedding\n",
    "V = tf.keras.layers.Embedding(len(word_to_id), 4)\n",
    "v = V(inputs)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight of center matrix\n",
    "V.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outer word embedding transpose\n",
    "U = tf.keras.layers.Dense(len(word_to_id), use_bias=False)\n",
    "vu = U(v)\n",
    "vu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight of outer matrix transpose\n",
    "U.get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WU = U.get_weights()[0]\n",
    "np.dot(v, WU[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp(uv) / sum(exp(uv))\n",
    "vu_prob = tf.nn.softmax(vu)\n",
    "vu_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outer word embedding transpose & softmax\n",
    "U = tf.keras.layers.Dense(len(word_to_id), use_bias=False, activation=tf.nn.softmax)\n",
    "vu_prob = U(v)\n",
    "vu_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label one hot\n",
    "vu_label = tf.one_hot(labels, len(word_to_id))\n",
    "vu_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross entroyp loss\n",
    "loss = - vu_label * tf.math.log(vu_prob)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse_categorical_crossentropy\n",
    "tf.keras.losses.sparse_categorical_crossentropy(labels, vu_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_vocab, d_model):\n",
    "    \"\"\"\n",
    "    skim gram 학습 모델\n",
    "    :param n_vocab: vocabulary 단어 수\n",
    "    :param d_model: 단어를 의미하는 벡터의 차원 수\n",
    "    \"\"\"\n",
    "    tokens = tf.keras.layers.Input(shape=(1,))\n",
    "\n",
    "    # center word vector\n",
    "    V = tf.keras.layers.Embedding(n_vocab, d_model)\n",
    "    v = V(tokens)\n",
    "    # 단어 예측 (activation=tf.nn.softmax)\n",
    "    U = tf.keras.layers.Dense(n_vocab, use_bias=False, activation=tf.nn.softmax)\n",
    "    vu_prob = U(v)\n",
    "\n",
    "    model = tf.keras.Model(inputs=tokens, outputs=vu_prob)\n",
    "    return model, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "model, V = build_model(len(word_to_id), 32)\n",
    "# 모델 내용 그래프 출력\n",
    "tf.keras.utils.plot_model(model, 'model.png', show_shapes=True)"
   ]
  },
  {
   "source": [
    "# 2. 데이터"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습할 말뭉치\n",
    "text = \"\"\"수학은 수식이 복잡해서 어렵다\n",
    "수학은 공식이 많아서 어렵다\n",
    "수학은 수식이 이해되면 쉽다\n",
    "수학은 공식이 능통하면 쉽다\n",
    "영어는 단어가 많아서 어렵다\n",
    "영어는 듣기가 복잡해서 어렵다\n",
    "영어는 단어가 이해되면 쉽다\n",
    "영어는 듣기가 능통하면 쉽다\n",
    "국어는 지문이 복잡해서 어렵다\n",
    "국어는 한문이 많아서 어렵다\n",
    "국어는 지문이 이해되면 쉽다\n",
    "국어는 한문이 능통하면 쉽다\"\"\""
   ]
  },
  {
   "source": [
    "# 3. Vocabulary"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 띄어쓰기 기준 단어 목록\n",
    "words = list(dict.fromkeys(text.split()))\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 단어별 고유한 번호 부여\n",
    "word_to_id = {'[PAD]': 0, '[UNK]': 1}\n",
    "for word in words:\n",
    "    word_to_id[word] = len(word_to_id)\n",
    "word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 숫자별 단어 부여\n",
    "id_to_word = {_id:word for word, _id in word_to_id.items()}\n",
    "id_to_word"
   ]
  },
  {
   "source": [
    "# 4. 학습용 데이터 생성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 줄바꿈 단위로 문장 분리\n",
    "sentences = text.split(\"\\n\")\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 띄어쓰기 단위로 단어 분리\n",
    "tokens = []\n",
    "for sentence in sentences:\n",
    "    tokens.append(sentence.split())\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center-outer 생성\n",
    "window_size = 2\n",
    "word_pairs = []\n",
    "for line_token in tokens:\n",
    "    for i in range(len(line_token)):\n",
    "        o_1 = max(0, i - window_size)\n",
    "        o_2 = min(len(line_token) - 1, i + window_size)\n",
    "        c = line_token[i]\n",
    "        word_pair = {\"c\": c, \"o\": [line_token[j] for j in range(o_1, o_2 + 1) if j != i]}\n",
    "        word_pairs.append(word_pair)\n",
    "print(len(word_pairs))\n",
    "word_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip gram dataset 생성\n",
    "train_tokens = []\n",
    "train_labels = []\n",
    "for word_pair in word_pairs:\n",
    "    c = word_pair[\"c\"]\n",
    "    o = word_pair[\"o\"]\n",
    "    for w in o:\n",
    "        # center word 입력\n",
    "        train_tokens.append(c)\n",
    "        # outer word 정답\n",
    "        train_labels.append(w)\n",
    "print(f\"tokens : {train_tokens}\")\n",
    "print(f\"labels : {train_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input token to id\n",
    "train_token_ids = np.array([word_to_id[token] for token in train_tokens])\n",
    "train_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label token to id\n",
    "train_label_ids = np.array([word_to_id[label] for label in train_labels])\n",
    "train_label_ids"
   ]
  },
  {
   "source": [
    "# 5. embedding 출력"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embdeeding(embedding, word_to_id):\n",
    "    \"\"\"\n",
    "    word의 embedding vector를 2차원 공간에서 위치를 표현 함\n",
    "    :param embedding: tf.keras.layers.Embedding 객체\n",
    "    :param word_to_id: word_to_id vocab\n",
    "    \"\"\"\n",
    "    # 폰트\n",
    "    font_name = \"NanumBarunGothic\"\n",
    "\n",
    "    # plot 크기 및 폰트 설정\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.rc('font', family=font_name)\n",
    "    plt.rcParams[\"axes.unicode_minus\"] = False # 한글 폰트 사용시 - 깨지는 문제 해결\n",
    "\n",
    "    # word와 vector 값 추출\n",
    "    vectors = []\n",
    "    words = []\n",
    "    for word, id in word_to_id.items():\n",
    "        if id < 2: continue  # 0: PAD, 1: UNK\n",
    "        vectors.append(embedding(id).numpy())\n",
    "        words.append(word)\n",
    "\n",
    "    # 2차원 보다 큰 경우 PCA를 이용해 2차원으로 차원 축소\n",
    "    if 2 < len(vectors[0]):\n",
    "        vectors = PCA().fit_transform(vectors)[:,:2]\n",
    "\n",
    "    # 벡터와 단어를 화면에 출력\n",
    "    for word, vector in zip(words, vectors):\n",
    "        plt.scatter(vector[0], vector[1])\n",
    "        plt.annotate(word, xy=(vector[0], vector[1]), xytext=(6, 4), textcoords='offset points', ha='right', va='bottom')\n",
    "\n",
    "    # 출력\n",
    "    plt.show()"
   ]
  },
  {
   "source": [
    "# 6. 학습"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 loss, optimizer, metric 정의\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embdeeding(V, word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "# 20번 반복 진행\n",
    "for i in range(20):\n",
    "    # epoch 학습 진행 후 vector를 화면에 출력\n",
    "    model.fit(train_token_ids, train_label_ids, batch_size=512, epochs=epochs, verbose=0)\n",
    "    print(f\"training >>> {(i+1) * epochs}\")\n",
    "    plot_embdeeding(V, word_to_id)"
   ]
  }
 ]
}