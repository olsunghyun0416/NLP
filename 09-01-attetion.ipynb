{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Install"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "source": [
    "# Evn"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import json\n",
    "import zipfile\n",
    "import math\n",
    "import copy\n",
    "import collections\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as KK\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed initialize\n",
    "random_seed = 1234\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google drive mount\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data dir\n",
    "data_dir = '/content/drive/MyDrive/Data/nlp'\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# songys chatbot dir\n",
    "songys_dir = os.path.join(data_dir, 'songys')\n",
    "if not os.path.exists(songys_dir):\n",
    "    os.makedirs(songys_dir)\n",
    "os.listdir(songys_dir)"
   ]
  },
  {
   "source": [
    "# Attention"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 문장\n",
    "sentences = [\n",
    "    ['나는 오늘 기분이 좋아', '네가 좋으니 나도 좋아'],\n",
    "    ['나는 오늘 행복해', '나도 행복하다'],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 문장을 띄어쓰기 단위로 분할\n",
    "words = []\n",
    "for pair in sentences:\n",
    "    for sentence in pair:\n",
    "        words.extend(sentence.split())\n",
    "\n",
    "# 중복 단어 제거\n",
    "words = list(dict.fromkeys(words))\n",
    "\n",
    "# 각 단어별 고유한 번호 부여\n",
    "word_to_id = {'[PAD]': 0, '[UNK]': 1, '[BOS]': 2, '[EOS]': 3}\n",
    "for word in words:\n",
    "    word_to_id[word] = len(word_to_id)\n",
    "\n",
    "# 각 숫자별 단어 부여\n",
    "id_to_word = {_id:word for word, _id in word_to_id.items()}\n",
    "\n",
    "word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question과 Answer를 숫자료\n",
    "question_list, answer_list = [], []\n",
    "\n",
    "for pair in sentences:\n",
    "    question_list.append([word_to_id[word] for word in pair[0].split()])\n",
    "    answer_list.append([word_to_id[word] for word in pair[1].split()])\n",
    "\n",
    "# 학습용 입력 데이터 생성\n",
    "train_enc_inputs, train_dec_inputs, train_labels = [], [], []\n",
    "\n",
    "for question, answer in zip(question_list, answer_list):\n",
    "    train_enc_inputs.append(question)\n",
    "    train_dec_inputs.append([word_to_id['[BOS]']] + answer)\n",
    "    train_labels.append(answer + [word_to_id['[EOS]']])\n",
    "\n",
    "# Encoder 입력의 길이를 모두 동일하게 변경 (최대길이 4)\n",
    "for row in train_enc_inputs:\n",
    "    row += [0] * (4 - len(row))\n",
    "\n",
    "# Decoder 입력의 길이를 모두 동일하게 변경 (최대길이 5)\n",
    "for row in train_dec_inputs:\n",
    "    row += [0] * (5 - len(row))\n",
    "\n",
    "# 정답의 길이를 모두 동일하게 변경 (최대길이 5)\n",
    "for row in train_labels:\n",
    "    row += [0] * (5 - len(row))\n",
    "\n",
    "# numpy array로 변환\n",
    "train_enc_inputs = np.array(train_enc_inputs)\n",
    "train_dec_inputs = np.array(train_dec_inputs)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "train_enc_inputs, train_dec_inputs, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random embed weight\n",
    "embed_weight = np.random.randint(-9, 9, (len(id_to_word), 5)) / 10\n",
    "\n",
    "# embedding 생성\n",
    "embedding = tf.keras.layers.Embedding(len(id_to_word), 5, weights=[embed_weight])\n",
    "\n",
    "# word embedding\n",
    "hidden_enc = embedding(train_enc_inputs)  # (bs, n_enc_seq, 5)\n",
    "hidden_dec = embedding(train_dec_inputs)  # (bs, n_dec_seq, 5)\n",
    "hidden_enc, hidden_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q, K, V 선언\n",
    "Q = hidden_dec\n",
    "K = hidden_enc\n",
    "V = K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (bs, n_seq, d_model)\n",
    "# Q[0][0] & K[0][0]\n",
    "q_idx, k_idx = 0, 0\n",
    "q_val = Q[0][q_idx]\n",
    "k_val = K[0][k_idx]\n",
    "print(q_val)\n",
    "print(k_val)\n",
    "print(np.dot(q_val, np.transpose(k_val)))\n",
    "print(tf.matmul([q_val], [k_val], transpose_b=True))\n",
    "print(np.sum(q_val * k_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q[0][0] & K[0][1]\n",
    "q_idx, k_idx = 0, 1\n",
    "q_val = Q[0][q_idx]\n",
    "k_val = K[0][k_idx]\n",
    "print(q_val)\n",
    "print(k_val)\n",
    "print(np.dot(q_val, np.transpose(k_val)))\n",
    "print(tf.matmul([q_val], [k_val], transpose_b=True))\n",
    "print(np.sum(q_val * k_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q[0][0] & K[0][1]\n",
    "q_idx, k_idx = 0, 1\n",
    "q_val = Q[0][q_idx]\n",
    "k_val = K[0][k_idx]\n",
    "print(q_val)\n",
    "print(k_val)\n",
    "print(np.dot(q_val, np.transpose(k_val)))\n",
    "print(tf.matmul([q_val], [k_val], transpose_b=True))\n",
    "print(np.sum(q_val * k_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q[0][0] & K[0][3]\n",
    "q_idx, k_idx = 0, 3\n",
    "q_val = Q[0][q_idx]\n",
    "k_val = K[0][k_idx]\n",
    "print(q_val)\n",
    "print(k_val)\n",
    "print(np.dot(q_val, np.transpose(k_val)))\n",
    "print(tf.matmul([q_val], [k_val], transpose_b=True))\n",
    "print(np.sum(q_val * k_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q[0][0] & K[0][*]\n",
    "attn_row = []\n",
    "q_val = Q[0][0]\n",
    "for k_idx in range(4):\n",
    "    k_val = K[0][k_idx]\n",
    "    attn_row.append(tf.matmul([q_val], [k_val], transpose_b=True).numpy()[0][0])\n",
    "attn_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q[0][*] & K[0][*]\n",
    "for q_idx in range(5):\n",
    "    attn_row = []\n",
    "    q_val = Q[1][q_idx]\n",
    "    for k_idx in range(4):\n",
    "        k_val = K[1][k_idx]\n",
    "        attn_row.append(tf.matmul([q_val], [k_val], transpose_b=True).numpy()[0][0])\n",
    "    print(attn_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention score\n",
    "attn_score = tf.matmul(Q, K, transpose_b=True).numpy()\n",
    "attn_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention prob\n",
    "attn_prob = tf.nn.softmax(attn_score, axis=-1).numpy()\n",
    "attn_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prob sum 확인\n",
    "np.sum(attn_prov, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attn_prob[0][0][0] & V[0][0]\n",
    "p_idx, v_idx = 0, 0\n",
    "p_val = attn_prob[0][0][p_idx]\n",
    "v_val = V[0][v_idx]\n",
    "print(p_val)\n",
    "print(v_val)\n",
    "attn_out_0_0_0 = p_val * v_val\n",
    "attn_out_0_0_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attn_prob[0][0][1] & V[0][1]\n",
    "p_idx, v_idx = 1, 1\n",
    "p_val = attn_prob[0][0][p_idx]\n",
    "v_val = V[0][v_idx]\n",
    "print(p_val)\n",
    "print(v_val)\n",
    "attn_out_0_0_1 = p_val * v_val\n",
    "attn_out_0_0_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attn_prob[0][0][2] & V[0][2]\n",
    "p_idx, v_idx = 2, 2\n",
    "p_val = attn_prob[0][0][p_idx]\n",
    "v_val = V[0][v_idx]\n",
    "print(p_val)\n",
    "print(v_val)\n",
    "attn_out_0_0_2 = p_val * v_val\n",
    "attn_out_0_0_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attn_prob[0][0][3] & V[0][3]\n",
    "p_idx, v_idx = 3, 3\n",
    "p_val = attn_prob[0][0][p_idx]\n",
    "v_val = V[0][v_idx]\n",
    "print(p_val)\n",
    "print(v_val)\n",
    "attn_out_0_0_3 = p_val * v_val\n",
    "attn_out_0_0_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attn_prob[0][0][*] & V[0][*]\n",
    "attn_out_0_0 = attn_out_0_0_0 + attn_out_0_0_1 + attn_out_0_0_2 + attn_out_0_0_3\n",
    "print(attn_out_0_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attn_prob[0][0][*] & V[0][*]\n",
    "attn_row = []\n",
    "for v_idx in range(4):\n",
    "    attn_row.append(attn_prov[0][0][v_idx] * V[0][v_idx])\n",
    "print(attn_row)\n",
    "print(np.sum(attn_row, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attn_prov[0][*] & V[0][*]\n",
    "for q_idx in range(5):\n",
    "    attn_row = []\n",
    "    for v_idx in range(4):\n",
    "        attn_row.append(attn_prov[1][q_idx][v_idx] * V[1][v_idx])\n",
    "    print(np.sum(attn_row, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attn_prob & V\n",
    "attn_out = tf.matmul(attn_prov, V)\n",
    "attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Q, K, V = inputs\n",
    "        #####################################\n",
    "        # attention score (dot-product)\n",
    "        attn_score = tf.matmul(Q, K, transpose_b=True)\n",
    "        # attention prov\n",
    "        attn_prob = tf.nn.softmax(attn_score)\n",
    "        # weighted sum\n",
    "        attn_out = tf.matmul(attn_prob, V)\n",
    "        #####################################\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = DotProductAttention()\n",
    "attention((Q, K, V))"
   ]
  },
  {
   "source": [
    "# Vocabulary & config"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab loading\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(os.path.join(data_dir, 'ko_32000.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(vocab)  # number of vocabulary\n",
    "n_enc_seq = 32  # number of sequence 1\n",
    "n_dec_seq = 40  # number of sequence 2\n",
    "d_model = 256  # dimension of model"
   ]
  },
  {
   "source": [
    "# 모델링"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word embedding\n",
    "hidden_enc = embedding(train_enc_inputs)  # (bs, n_enc_seq, 5)\n",
    "hidden_dec = embedding(train_dec_inputs)  # (bs, n_dec_seq, 5)\n",
    "hidden_enc, hidden_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder lstm\n",
    "fw_cell = tf.keras.layers.LSTM(units=4, return_state=True, return_sequences=True)\n",
    "bw_cell = tf.keras.layers.LSTM(units=4, go_backwards=True, return_state=True, return_sequences=True)\n",
    "lstm_enc = tf.keras.layers.Bidirectional(fw_cell, backward_layer=bw_cell)\n",
    "\n",
    "hidden_enc, fw_h, fw_c, bw_h, bw_c = lstm_enc(hidden_enc)  # (bs, d_model * 2), (bs, d_model), (bs, d_model), (bs, d_model), (bs, d_model)\n",
    "hidden_enc.shape, fw_h.shape, fw_c.shape, bw_h.shape, bw_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat hidden & cell\n",
    "s_h = tf.concat([fw_h, bw_h], axis=-1)  # (bs, d_model * 2)\n",
    "s_c = tf.concat([fw_c, bw_c], axis=-1)  # (bs, d_model * 2)\n",
    "s_h.shape, s_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder LSTM\n",
    "lstm_dec = tf.keras.layers.LSTM(units=8, return_sequences=True)\n",
    "hidden_dec = lstm_dec(hidden_dec, initial_state=[s_h, s_c])  # (bs, n_dec_seq, d_model)\n",
    "hidden_dec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention\n",
    "attention = DotProductAttention()\n",
    "attn_out = attention((hidden_dec, hidden_enc, hidden_enc))\n",
    "attn_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat output\n",
    "dec_out = tf.concat([attn_out, hidden_dec], axis=-1)\n",
    "dec_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다음단어 예측\n",
    "dense_out = tf.keras.layers.Dense(units=len(word_to_id), activation=tf.nn.softmax)\n",
    "outputs = dense_out(dec_out)  # (bs, n_dec_seq, n_vocab)\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_vocab, d_model, n_enc_seq, n_dec_seq):\n",
    "    \"\"\"\n",
    "    seq2seq 모델\n",
    "    :param n_vocab: vocabulary 단어 수\n",
    "    :param d_model: 단어를 의미하는 벡터의 차원 수\n",
    "    :param n_enc_seq: encoder 문장 길이 (단어 수)\n",
    "    :param n_dec_seq: decoder 문장 길이 (단어 수)\n",
    "    \"\"\"\n",
    "    inputs_enc = tf.keras.layers.Input((n_enc_seq,))  # (bs, n_enc_seq)\n",
    "    inputs_dec = tf.keras.layers.Input((n_dec_seq,))  # (bs, n_dec_seq)\n",
    "    ################################################\n",
    "    # 입력 단어를 vector로 변환\n",
    "    embedding = tf.keras.layers.Embedding(n_vocab, d_model)\n",
    "    hidden_enc = embedding(inputs_enc)  # (bs, n_enc_seq, d_model)\n",
    "    hidden_dec = embedding(inputs_dec)  # (bs, n_dec_seq, d_model)\n",
    "\n",
    "    # Encoder LSTM (uni-direction, bi-direction 가능)\n",
    "    fw_cell = tf.keras.layers.LSTM(units=d_model, return_state=True, return_sequences=True)\n",
    "    bw_cell = tf.keras.layers.LSTM(units=d_model, go_backwards=True, return_state=True, return_sequences=True)\n",
    "    lstm_enc = tf.keras.layers.Bidirectional(fw_cell, backward_layer=bw_cell)\n",
    "    hidden_enc, fw_h, fw_c, bw_h, bw_c = lstm_enc(hidden_enc)  # (bs, n_enc_seq, d_model * 2), (bs, d_model), (bs, d_model), (bs, d_model), (bs, d_model)\n",
    "\n",
    "    # Concatenate hidden states and cell states\n",
    "    s_h = tf.concat([fw_h, bw_h], axis=-1)  # (bs, d_model * 2)\n",
    "    s_c = tf.concat([fw_c, bw_c], axis=-1)  # (bs, d_model * 2)\n",
    "\n",
    "    # Decoder LSTM (uni-direction만 가능)\n",
    "    lstm_dec = tf.keras.layers.LSTM(units=d_model * 2, return_sequences=True)\n",
    "    hidden_dec = lstm_dec(hidden_dec, initial_state=[s_h, s_c])  # (bs, n_dec_seq, d_model)\n",
    "\n",
    "    # attention\n",
    "    attention = DotProductAttention()\n",
    "    attn_out = attention((hidden_dec, hidden_enc, hidden_enc))\n",
    "\n",
    "    # concat output\n",
    "    dec_out = tf.concat([attn_out, hidden_dec], axis=-1)\n",
    "    \n",
    "    # 다음단어 예측\n",
    "    dense_out = tf.keras.layers.Dense(units=n_vocab, activation=tf.nn.softmax)\n",
    "    outputs = dense_out(dec_out)  # (bs, n_dec_seq, n_vocab)\n",
    "    ################################################\n",
    "    # 학습할 모델 선언\n",
    "    model = tf.keras.Model(inputs=(inputs_enc, inputs_dec), outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "model = build_model(len(vocab), d_model, n_enc_seq, n_dec_seq)\n",
    "# 모델 내용 그래프 출력\n",
    "tf.keras.utils.plot_model(model, 'model.png', show_shapes=True)"
   ]
  },
  {
   "source": [
    "# Loss & Acc"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    pad 부분을 제외하고 loss를 계산하는 함수\n",
    "    :param y_true: 정답\n",
    "    :param y_pred: 예측 값\n",
    "    :retrun loss: pad 부분이 제외된 loss 값\n",
    "    \"\"\"\n",
    "    # loss = sparse_entropy = tf.keras.losses.SparseCategoricalCrossentropy()(y_true, y_pred)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)(y_true, y_pred)\n",
    "    mask = tf.not_equal(y_true, 0)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    # print(mask)\n",
    "    loss *= mask\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    pad 부분을 제외하고 accuracy를 계산하는 함수\n",
    "    :param y_true: 정답\n",
    "    :param y_pred: 예측 값\n",
    "    :retrun loss: pad 부분이 제외된 accuracy 값\n",
    "    \"\"\"\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    # print(y_true)\n",
    "    y_pred_class = tf.cast(tf.argmax(y_pred, axis=-1), tf.float32)\n",
    "    # print(y_pred_class)\n",
    "    matches = tf.cast(tf.equal(y_true, y_pred_class), tf.float32)\n",
    "    # print(matches)\n",
    "    mask = tf.not_equal(y_true, 0)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    # print(mask)\n",
    "    matches *= mask\n",
    "    # print(matches)\n",
    "    # accuracy = tf.reduce_sum(matches) / tf.maximum(tf.reduce_sum(tf.ones_like(matches)), 1)\n",
    "    accuracy = tf.reduce_sum(matches) / tf.maximum(tf.reduce_sum(mask), 1)\n",
    "    return accuracy"
   ]
  },
  {
   "source": [
    "# Sample Data Project"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 1. Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(os.path.join(songys_dir, 'ChatbotData.csv'))\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.dropna()\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤하게 10개만 확인\n",
    "df_train = df_train.sample(10)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(df, n_enc_seq, n_dec_seq):\n",
    "    \"\"\"\n",
    "    Quora 학습 데이터 생성\n",
    "    :param df: data frame\n",
    "    :param n_enc_seq: number of encoder sequence\n",
    "    :param n_dec_seq: number of decoder sequence\n",
    "    :return enc_inputs: encoder input data\n",
    "    :return dec_inputs: decoder input data\n",
    "    :return labels: label data\n",
    "    \"\"\"\n",
    "    n_enc_max = n_enc_seq\n",
    "    n_dec_max = n_dec_seq - 1\n",
    "    enc_inputs = np.zeros((len(df), n_enc_seq)).astype(np.int32)\n",
    "    dec_inputs = np.zeros((len(df), n_dec_seq)).astype(np.int32)\n",
    "    labels = np.zeros((len(df), n_dec_seq)).astype(np.int32)\n",
    "    index = 0\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        # tokens 저장\n",
    "        print()\n",
    "        Q = row['Q']\n",
    "        A = row['A']\n",
    "        print(Q, '/', A)\n",
    "\n",
    "        tokens_q = vocab.encode_as_pieces(Q)\n",
    "        print(len(tokens_q), ':', tokens_q)\n",
    "        tokens_a = vocab.encode_as_pieces(A)\n",
    "        print(len(tokens_a), ':', tokens_a)\n",
    "\n",
    "        tokens_ids_q = vocab.encode_as_ids(Q)[:n_enc_max]\n",
    "        print(len(tokens_ids_q), ':', tokens_ids_q)\n",
    "        tokens_ids_a = vocab.encode_as_ids(A)[:n_dec_max]\n",
    "        print(len(tokens_ids_a), ':', tokens_ids_a)\n",
    "\n",
    "        tokens_dec_in = [vocab.bos_id()] + tokens_ids_a\n",
    "        tokens_dec_out = tokens_ids_a + [vocab.eos_id()]\n",
    "\n",
    "        tokens_ids_q += [0] * (n_enc_seq - len(tokens_ids_q))\n",
    "        print(len(tokens_ids_q), ':', tokens_ids_q)\n",
    "        tokens_dec_in += [0] * (n_dec_seq - len(tokens_dec_in))\n",
    "        print(len(tokens_dec_in), ':', tokens_dec_in)\n",
    "        tokens_dec_out += [0] * (n_dec_seq - len(tokens_dec_out))\n",
    "        print(len(tokens_dec_out), ':', tokens_dec_out)\n",
    "\n",
    "        enc_inputs[index] = tokens_ids_q\n",
    "        dec_inputs[index] = tokens_dec_in\n",
    "        labels[index] = tokens_dec_out\n",
    "        index += 1\n",
    "    return enc_inputs, dec_inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data 생성\n",
    "train_enc_inputs, train_dec_inputs, train_labels = load_data(df_train, n_enc_seq, n_dec_seq)\n",
    "train_enc_inputs, train_dec_inputs, train_labels"
   ]
  },
  {
   "source": [
    "# 2. 학습"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "model = build_model(len(vocab), d_model, n_enc_seq, n_dec_seq)\n",
    "# 모델 내용 그래프 출력\n",
    "tf.keras.utils.plot_model(model, 'model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 loss, optimizer, metric 정의\n",
    "model.compile(loss=lm_loss, optimizer='adam', metrics=[lm_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='lm_acc', patience=100)\n",
    "# save weights callback\n",
    "save_weights = tf.keras.callbacks.ModelCheckpoint(os.path.join(songys_dir, 'lstm_dot.hdf5'),\n",
    "                                                  monitor='lm_acc',\n",
    "                                                  verbose=1,\n",
    "                                                  save_best_only=True,\n",
    "                                                  mode=\"max\",\n",
    "                                                  save_freq=\"epoch\",\n",
    "                                                  save_weights_only=True)\n",
    "# csv logger\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(os.path.join(songys_dir, 'lstm_dot.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "history = model.fit((train_enc_inputs, train_dec_inputs),\n",
    "                    train_labels,\n",
    "                    epochs=400,\n",
    "                    batch_size=256,\n",
    "                    callbacks=[early_stopping, save_weights, csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], 'b-', label='loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['lm_acc'], 'g-', label='acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "# 3. Inference"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "model = build_model(len(vocab), d_model, n_enc_seq, n_dec_seq)\n",
    "# train weight로 초기화\n",
    "model.load_weights(os.path.join(songys_dir, 'lstm_dot.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_chat(vocab, model, n_enc_seq, n_dec_seq, string):\n",
    "    \"\"\"\n",
    "    seq2seq chat\n",
    "    :param vocab: vocab\n",
    "    :param model: model\n",
    "    :param n_enc_seq: number of enc seqence\n",
    "    :param n_dec_seq: number of dec seqence\n",
    "    :param string: inpust string\n",
    "    \"\"\"\n",
    "    # qeustion\n",
    "    q = vocab.encode_as_pieces(string)\n",
    "    q_id = [vocab.piece_to_id(p) for p in q][:n_enc_seq]\n",
    "    q_id += [0] * (n_enc_seq - len(q_id))\n",
    "    assert len(q_id) == n_enc_seq\n",
    "\n",
    "    # answer\n",
    "    a_id = [vocab.bos_id()]\n",
    "    a_id += [0] * (n_dec_seq - len(a_id))\n",
    "    assert len(a_id) == n_dec_seq\n",
    "\n",
    "    # 처음부터 예측\n",
    "    start_idx = 0\n",
    "\n",
    "    for _ in range(start_idx, n_dec_seq - 1):\n",
    "        outputs = model.predict((np.array([q_id]), np.array([a_id])))\n",
    "        prob = outputs[0][start_idx]\n",
    "        word_id = np.argmax(prob)\n",
    "        if word_id == vocab.eos_id():\n",
    "            break\n",
    "        a_id[start_idx + 1] = int(word_id)\n",
    "        start_idx += 1\n",
    "    predict_id = a_id[1:start_idx + 1]\n",
    "    predict_str = vocab.decode_ids(predict_id)\n",
    "    return predict_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    string = input('질문 > ')\n",
    "    string = string.strip()\n",
    "    if len(string) == 0:\n",
    "        break\n",
    "    predict_str = do_chat(vocab, model, n_enc_seq, n_dec_seq, string)\n",
    "    print(f'답변 > {predict_str}')"
   ]
  },
  {
   "source": [
    "# All Data Project"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 1. Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(os.path.join(songys_dir, 'ChatbotData.csv'))\n",
    "print(len(df_train))\n",
    "df_train = df_train.dropna()\n",
    "print(len(df_train))\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(df, n_enc_seq, n_dec_seq):\n",
    "    \"\"\"\n",
    "    Quora 학습 데이터 생성\n",
    "    :param df: data frame\n",
    "    :param n_enc_seq: number of encoder sequence\n",
    "    :param n_dec_seq: number of decoder sequence\n",
    "    :return enc_inputs: encoder input data\n",
    "    :return dec_inputs: decoder input data\n",
    "    :return labels: label data\n",
    "    \"\"\"\n",
    "    n_enc_max = n_enc_seq\n",
    "    n_dec_max = n_dec_seq - 1\n",
    "    enc_inputs = np.zeros((len(df), n_enc_seq)).astype(np.int32)\n",
    "    dec_inputs = np.zeros((len(df), n_dec_seq)).astype(np.int32)\n",
    "    labels = np.zeros((len(df), n_dec_seq)).astype(np.int32)\n",
    "    index = 0\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        # tokens 저장\n",
    "        Q = row['Q']\n",
    "        A = row['A']\n",
    "\n",
    "        tokens_q = vocab.encode_as_pieces(Q)\n",
    "        tokens_a = vocab.encode_as_pieces(A)\n",
    "\n",
    "        tokens_ids_q = vocab.encode_as_ids(Q)[:n_enc_max]\n",
    "        tokens_ids_a = vocab.encode_as_ids(A)[:n_dec_max]\n",
    "\n",
    "        tokens_dec_in = [vocab.bos_id()] + tokens_ids_a\n",
    "        tokens_dec_out = tokens_ids_a + [vocab.eos_id()]\n",
    "\n",
    "        tokens_ids_q += [0] * (n_enc_seq - len(tokens_ids_q))\n",
    "        tokens_dec_in += [0] * (n_dec_seq - len(tokens_dec_in))\n",
    "        tokens_dec_out += [0] * (n_dec_seq - len(tokens_dec_out))\n",
    "\n",
    "        enc_inputs[index] = tokens_ids_q\n",
    "        dec_inputs[index] = tokens_dec_in\n",
    "        labels[index] = tokens_dec_out\n",
    "        index += 1\n",
    "    return enc_inputs, dec_inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data 생성\n",
    "train_enc_inputs, train_dec_inputs, train_labels = load_data(df_train, n_enc_seq, n_dec_seq)\n",
    "train_enc_inputs, train_dec_inputs, train_labels"
   ]
  },
  {
   "source": [
    "# 2. 학습"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "model = build_model(len(vocab), d_model, n_enc_seq, n_dec_seq)\n",
    "# 모델 내용 그래프 출력\n",
    "tf.keras.utils.plot_model(model, 'model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 loss, optimizer, metric 정의\n",
    "model.compile(loss=lm_loss, optimizer='adam', metrics=[lm_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='lm_acc', patience=5)\n",
    "# save weights callback\n",
    "save_weights = tf.keras.callbacks.ModelCheckpoint(os.path.join(songys_dir, 'lstm_dot.hdf5'),\n",
    "                                                  monitor='lm_acc',\n",
    "                                                  verbose=1,\n",
    "                                                  save_best_only=True,\n",
    "                                                  mode=\"max\",\n",
    "                                                  save_freq=\"epoch\",\n",
    "                                                  save_weights_only=True)\n",
    "# csv logger\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(os.path.join(songys_dir, 'lstm_dot.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "history = model.fit((train_enc_inputs, train_dec_inputs),\n",
    "                    train_labels,\n",
    "                    epochs=100,\n",
    "                    batch_size=256,\n",
    "                    callbacks=[early_stopping, save_weights, csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], 'b-', label='loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['lm_acc'], 'g-', label='acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "# Inference"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "model = build_model(len(vocab), d_model, n_enc_seq, n_dec_seq)\n",
    "# train weight로 초기화\n",
    "model.load_weights(os.path.join(songys_dir, 'lstm_dot.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_chat(vocab, model, n_enc_seq, n_dec_seq, string):\n",
    "    \"\"\"\n",
    "    seq2seq chat\n",
    "    :param vocab: vocab\n",
    "    :param model: model\n",
    "    :param n_enc_seq: number of enc seqence\n",
    "    :param n_dec_seq: number of dec seqence\n",
    "    :param string: inpust string\n",
    "    \"\"\"\n",
    "    # qeustion\n",
    "    q = vocab.encode_as_pieces(string)\n",
    "    q_id = [vocab.piece_to_id(p) for p in q][:n_enc_seq]\n",
    "    q_id += [0] * (n_enc_seq - len(q_id))\n",
    "    assert len(q_id) == n_enc_seq\n",
    "\n",
    "    # answer\n",
    "    a_id = [vocab.bos_id()]\n",
    "    a_id += [0] * (n_dec_seq - len(a_id))\n",
    "    assert len(a_id) == n_dec_seq\n",
    "\n",
    "    # 처음부터 예측\n",
    "    start_idx = 0\n",
    "\n",
    "    for _ in range(start_idx, n_dec_seq - 1):\n",
    "        outputs = model.predict((np.array([q_id]), np.array([a_id])))\n",
    "        prob = outputs[0][start_idx]\n",
    "        word_id = np.argmax(prob)\n",
    "        if word_id == vocab.eos_id():\n",
    "            break\n",
    "        a_id[start_idx + 1] = int(word_id)\n",
    "        start_idx += 1\n",
    "    predict_id = a_id[1:start_idx + 1]\n",
    "    predict_str = vocab.decode_ids(predict_id)\n",
    "    return predict_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    string = input('질문 > ')\n",
    "    string = string.strip()\n",
    "    if len(string) == 0:\n",
    "        break\n",
    "    predict_str = do_chat(vocab, model, n_enc_seq, n_dec_seq, string)\n",
    "    print(f'답변 > {predict_str}')"
   ]
  }
 ]
}